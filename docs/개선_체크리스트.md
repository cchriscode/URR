# ✅ 프로젝트 개선 체크리스트

> **프로젝트**: 대규모 동시 접속 티켓 예매 플랫폼
> **개선 전**: TIKETI (Node.js + React)
> **개선 후**: URR (Spring Boot + Next.js)

---

## 1. 개선 전 작업물

### 스크린샷 이미지

> 📸 `project-ticketing/` 프로젝트 구조 스크린샷 첨부 위치

**프로젝트 디렉토리 구조:**
```
project-ticketing/
├── backend/                   ← Express 모놀리식 Gateway (server.js 248줄)
│   └── src/
│       ├── routes/            ← 프록시 라우트 + _legacy_backup/ 폴더 잔존
│       ├── middleware/        ← auth.js (JWT 검증 - 중복 코드 원본)
│       └── services/          ← 백그라운드 태스크 (일부 주석 처리됨)
├── services/                  ← Node.js MSA 4개
│   ├── auth-service/          ← JWT + Google OAuth
│   ├── ticket-service/        ← 이벤트, 좌석, 예매
│   ├── payment-service/       ← 토스페이먼츠 결제
│   └── stats-service/         ← 통계
├── services-spring/           ← ⚠️ 빈 디렉토리 (계획만 존재)
│   ├── auth-service/          ← 빈 폴더
│   ├── gateway-service/       ← 빈 폴더
│   ├── ticket-service/        ← 빈 폴더
│   └── ...
├── frontend/                  ← React 18.2 CRA (JavaScript)
│   └── src/pages/             ← 28개 페이지 (각각 .js + .css 파일)
├── packages/                  ← npm 워크스페이스 공유 코드
├── k8s/                       ← 번호 기반 flat YAML (00~ 14번)
├── monitoring/                ← Prometheus/Grafana 설정 5개 파일
└── .github/workflows/         ← CI/CD 5개 (개별 작성, 중복)
```

### 작업물 설명

TIKETI는 콘서트, 뮤지컬 등 이벤트 티켓을 실시간으로 예매할 수 있는 플랫폼이다. Node.js(Express) 기반 API Gateway와 4개 마이크로서비스, React SPA 프론트엔드로 구성되었다.

**기술 스택:**

| 레이어 | 기술 |
|--------|------|
| 프론트엔드 | React 18.2 (CRA), JavaScript, React Router 6, Socket.IO |
| 백엔드 Gateway | Node.js 18 + Express 4.18 (모놀리식 server.js 248줄) |
| 백엔드 MSA | Node.js 서비스 4개 (auth, ticket, payment, stats) |
| 데이터베이스 | PostgreSQL 15 단일 인스턴스 (스키마 분리) |
| 캐시/큐 | DragonflyDB (Redis 호환), Redis Pub/Sub |
| 인프라 | Kind 로컬 K8s, 번호 기반 YAML 매니페스트 15개 |
| CI/CD | GitHub Actions 워크플로우 5개 (서비스별 개별, 재사용 없음) |
| 모니터링 | Prometheus + Grafana + Loki + Promtail |

**주요 문제점:**

1. **코드 중복**: 인증 미들웨어(`auth.js`)가 `backend/`, `services/ticket-service/`, `services/payment-service/`, `services/stats-service/` 4곳에 거의 동일한 코드로 복사-붙여넣기됨. 한 곳을 수정하면 나머지 3곳도 수동으로 수정 필요
2. **보안 취약**: JWT_SECRET에 하드코딩된 기본값(`'dev-only-secret-change-in-production-...'`)이 3개 서비스에 노출. Rate Limiting, VWR 대기열, 헤더 스푸핑 방지 모두 없음
3. **테스트 부재**: 백엔드 단위 테스트 0개, 부하 테스트 없음
4. **인프라 미비**: Terraform 없음, NetworkPolicy 없음, Staging 환경 없음, Blue-Green 배포 없음
5. **프론트엔드 한계**: React CRA + JavaScript → SSR 불가, 타입 안전성 없음, 페이지별 CSS 충돌 위험
6. **유사 MSA**: 단일 PostgreSQL에 스키마만 분리 → 진정한 DB 격리가 아님
7. **로깅**: `console.log()`/`console.error()` 362건 → 구조화되지 않은 로그

---

## 2. 받은 피드백 정리

### 피드백 제공자 정보

- **제공자**: 프로젝트 코드 리뷰 및 아키텍처 감사 (팀 내부 리뷰 + 멘토 피드백)
- **대상**: TIKETI 프로젝트 전체 (백엔드, 프론트엔드, 인프라, CI/CD, 보안)

### 피드백 받은 날짜/방식

- **일자**: 2026년 2월 초
- **방식**: 코드 리뷰 (GitHub PR), 아키텍처 감사 보고서, 1:1 멘토링

### 피드백 내용 요약

#### 피드백 1: 백엔드 아키텍처 전면 재설계 필요

> "Node.js 4개 서비스로는 동시 접속 수만 명 규모의 티켓팅을 감당하기 어렵다. Spring Boot(Java 21) 기반으로 전환하고 도메인별 서비스를 더 세분화해야 한다. 현재 Express Gateway의 248줄 모놀리식 server.js에 라우팅, 인증, WebSocket, 메트릭이 혼재되어 있어 관심사 분리가 안 된다."

- 4개 서비스 → 8개로 분리 (catalog, community, queue 신규 추가 필요)
- Spring Cloud Gateway로 전용 API Gateway 구축
- Kafka 기반 비동기 이벤트 처리 도입

#### 피드백 2: 보안 체계 근본적 보강 필요

> "JWT를 각 서비스에서 개별 검증하면 시크릿 유출 범위가 넓어진다. Gateway에서 중앙 검증하고 X-User-* 헤더로 전파하는 패턴으로 바꿔야 한다. Refresh Token 로테이션도 없고, Rate Limiting도 없어 보안 관점에서 프로덕션에 올리기 어렵다."

- Gateway 중앙 JWT 검증 + X-User-* 헤더 스푸핑 방지
- Refresh Token 로테이션 + 탈취 탐지
- Redis Lua 기반 경로별 Rate Limiting
- VWR(Virtual Waiting Room) 2-Tier 대기열 시스템
- 서비스 간 내부 토큰 timing-safe 비교

#### 피드백 3: 테스트 없이는 CI/CD가 무의미

> "백엔드 단위 테스트가 0개인 상태에서 CI/CD 파이프라인은 빌드만 확인할 뿐 품질 게이트 역할을 못 한다. 핵심 비즈니스 로직에 대한 단위 테스트를 작성하고, 통합 테스트와 분리해야 한다."

- 각 서비스 핵심 로직 단위 테스트 작성
- `@Tag("integration")`으로 단위/통합 테스트 분리
- k6 기반 부하 테스트 도입

#### 피드백 4: 인프라를 코드로 관리해야 한다

> "AWS 인프라가 코드로 관리되지 않으면 환경 재현이 불가능하고, 장애 시 복구가 어렵다. Terraform으로 VPC, EKS, RDS 등을 모듈화하고, K8s 매니페스트도 Kustomize overlay로 환경별 분리해야 한다."

- Terraform 모듈화 (VPC, EKS, RDS, ElastiCache, MSK 등)
- Kustomize base/overlay (Kind, Dev, Staging, Prod)
- NetworkPolicy default-deny + whitelist
- Argo Rollouts Blue-Green 배포

#### 피드백 5: CI/CD 워크플로우 중복 제거

> "5개 워크플로우에 250줄 이상 중복 코드가 있다. 재사용 가능한 워크플로우 템플릿으로 통합하고, PR 시 변경된 서비스만 테스트하는 매트릭스 전략을 도입해야 한다."

- `reusable-spring-ci-cd.yml` 재사용 템플릿
- `pr-validation.yml` path-based 매트릭스
- 롤백 워크플로우, 보안 스캔(Trivy), Discord 알림

#### 피드백 6: 프론트엔드 현대화

> "React CRA + JavaScript 조합은 SSR 불가, 타입 안전성 부재, 빌드 최적화 한계가 있다. Next.js + TypeScript + Tailwind CSS로 전환하면 SEO, 성능, 개발 생산성이 크게 개선된다."

- Next.js App Router + TypeScript
- Tailwind CSS v4
- TanStack Query v5 서버 상태 관리

---

## 3. 개선 후 작업물

### 개선된 결과물 이미지/내용

> 📸 `URR/` 프로젝트 구조 스크린샷 첨부 위치

**개선된 프로젝트 디렉토리 구조:**
```
URR/
├── apps/web/                     ← Next.js 16 + TypeScript + Tailwind CSS v4
│   └── src/
│       ├── app/                  ← App Router (SSR/SSG)
│       ├── components/           ← 재사용 컴포넌트
│       └── hooks/                ← 커스텀 훅 (use-countdown, use-queue-polling 등)
├── services-spring/              ← Spring Boot 3.5.0 + Java 21 (8개 MSA)
│   ├── auth-service/             ← JWT + Google OAuth + Refresh Token 로테이션
│   ├── gateway-service/          ← Spring Cloud Gateway (Rate Limit, VWR, JWT 필터)
│   ├── ticket-service/           ← 예매, 좌석, 양도, 멤버십
│   ├── payment-service/          ← 결제 (Toss 연동), Kafka Producer
│   ├── stats-service/            ← 통계, Kafka Consumer
│   ├── queue-service/            ← VWR 대기열, Redis ZSET, SQS
│   ├── catalog-service/          ← 이벤트/아티스트 카탈로그, S3       ★ 신규
│   ├── community-service/        ← 커뮤니티, 뉴스, 댓글              ★ 신규
│   └── urr-common/               ← 공유 라이브러리 (7개 컴포넌트)     ★ 신규
├── terraform/                    ← IaC 20개 모듈                     ★ 신규
│   ├── modules/ (20개)           ← VPC, EKS, RDS, MSK, CloudFront, WAF...
│   └── environments/             ← staging/, prod/
├── k8s/spring/                   ← Kustomize base/overlay
│   ├── base/                     ← 공통 매니페스트 + NetworkPolicy
│   └── overlays/                 ← kind/, dev/, staging/, prod/
├── k8s/argo-rollouts/            ← Blue-Green 배포                   ★ 신규
├── .github/workflows/            ← CI/CD 14개 (재사용 템플릿 기반)
│   ├── reusable-spring-ci-cd.yml ← 재사용 템플릿 (8개 서비스 공유)   ★ 신규
│   ├── pr-validation.yml         ← 변경 서비스만 자동 테스트          ★ 신규
│   └── rollback.yml              ← 롤백 워크플로우                    ★ 신규
├── tests/                        ← 부하/카오스 테스트 (k6)            ★ 신규
└── docs/analyze/                 ← 분석 문서 16개                     ★ 신규
```

**개선된 아키텍처:**
```
사용자 → CloudFront (WAF + Lambda@Edge)
           → AWS ALB
             → Spring Cloud Gateway MVC
                [RateLimitFilter → VwrEntryTokenFilter → CookieAuthFilter → JwtAuthFilter]
                  ├── auth-service        (JWT, OAuth, Refresh Token 로테이션)
                  ├── ticket-service      (예매, 좌석, 양도, 멤버십)
                  ├── payment-service     (결제, Kafka Producer)
                  ├── stats-service       (통계, Kafka Consumer)
                  ├── queue-service       (VWR 대기열, Redis + SQS)
                  ├── catalog-service     (카탈로그, S3 이미지)     ★ 신규
                  ├── community-service   (커뮤니티, 뉴스)          ★ 신규
                  └── urr-common          (공유 라이브러리)          ★ 신규
```

**수치 비교:**

| 항목 | TIKETI (개선 전) | URR (개선 후) |
|------|-----------------|--------------|
| 백엔드 언어 | Node.js (JavaScript) | Java 21 (Spring Boot 3.5.0) |
| 마이크로서비스 수 | 4개 | **8개** |
| 백엔드 소스 파일 | JS 40개 | **Java 193개** |
| 단위 테스트 | 0개 | **19개** |
| CI/CD 워크플로우 | 5개 (중복) | **14개** (재사용 기반) |
| Terraform 모듈 | 0개 | **20개** |
| K8s 환경 | 1개 (Kind) | **4개** (Kind/Dev/Staging/Prod) |
| 프론트엔드 | React CRA (JS) | **Next.js 16 (TypeScript)** |
| Rate Limiting | 없음 | **4단계 경로별** |
| 대기열(VWR) | 없음 | **2-Tier (100만+ 동시)** |
| NetworkPolicy | 없음 | **default-deny + whitelist** |
| 배포 전략 | kubectl apply | **Argo Rollouts Blue-Green** |

### 개선 과정 단계별 설명

#### 단계 1: 백엔드 기술 전환 (Node.js → Spring Boot)

**문제:**
TIKETI의 Express Gateway는 `server.js` 248줄에 라우팅, 인증, WebSocket, 메트릭이 혼재되어 있었고, 4개 Node.js 서비스는 인증 미들웨어가 4곳에 복사-붙여넣기되어 있었다.

**해결:**
- Node.js → **Java 21 + Spring Boot 3.5.0** 전환
- 4개 서비스 → **8개 마이크로서비스** (catalog, community, queue 신규)
- `packages/` npm 워크스페이스 → **`urr-common/` Gradle Composite Build**
- 복사-붙여넣기 인증 코드 → **urr-common의 `JwtTokenParser`** 1곳으로 통합
- Socket.IO → **Kafka 4개 토픽** (payment-events, reservation-events, transfer-events, membership-events)
- 단일 DB 스키마 분리 → **서비스별 독립 PostgreSQL 인스턴스**

**urr-common 공유 라이브러리 구성 (6개 서비스가 공유):**

| 컴포넌트 | 역할 |
|----------|------|
| `JwtTokenParser` | X-User-* 헤더에서 인증 정보 추출 (JWT 시크릿 불필요) |
| `GlobalExceptionHandler` | 전 서비스 공통 예외 처리 |
| `InternalTokenValidatorAutoConfiguration` | 서비스 간 내부 토큰 검증 (조건부 빈) |
| `DataSourceRoutingConfig` | Primary/Replica 읽기-쓰기 분리 |
| `ReadWriteRoutingDataSource` | 트랜잭션 readOnly 플래그 기반 DB 라우팅 |
| `AuthUser` | 인증 사용자 정보 record |
| `PreSaleSchedule` | 멤버십 티어별 선예매 일정 계산 |

#### 단계 2: 보안 체계 강화

**문제:**
TIKETI에서는 JWT를 각 서비스가 개별 검증(매번 DB 조회)하고, JWT_SECRET 기본값이 3곳에 하드코딩되어 있었으며, Rate Limiting, VWR, 헤더 스푸핑 방지가 전혀 없었다.

**해결:**

| 보안 기능 | TIKETI (문제) | URR (해결) |
|-----------|--------------|-----------|
| JWT 검증 | 각 서비스에서 개별 검증 + 매번 DB 조회 | **Gateway 중앙 검증** → X-User-* 헤더 전파 |
| JWT 시크릿 | 3곳에 하드코딩 기본값 노출 | Gateway만 시크릿 보유, 다운스트림 불필요 |
| Refresh Token | 로테이션 없음 (탈취 시 무제한 사용) | **Family-based reuse detection** + 토큰 로테이션 |
| Rate Limiting | 없음 | Redis Lua 기반 4단계 (AUTH 60, BOOKING 30, QUEUE 120, GENERAL 3000 rpm) |
| 대기열 | 없음 (서버 직접 도달) | **2-Tier VWR**: Lambda/DynamoDB(100만+) → Redis ZSET(10만) |
| 헤더 스푸핑 | 방지 없음 | JwtAuthFilter에서 **X-User-* 헤더 strip 후 검증된 값만 주입** |
| 내부 API 토큰 | `===` 일반 비교 | **`MessageDigest.isEqual()` timing-safe** 비교 |
| 쿠키 | 기본 설정 | **HttpOnly, Secure, SameSite=Lax**, Refresh Token Path 분리(`/api/auth`) |
| 네트워크 | Pod 간 무제한 통신 | K8s **NetworkPolicy default-deny** + 서비스별 whitelist |
| WAF | 없음 | **AWS WAFv2** (4 규칙) + CloudFront |

#### 단계 3: 테스트 체계 구축

**문제:**
TIKETI는 백엔드 단위 테스트가 0개였다. CI/CD가 빌드 성공만 확인할 뿐 품질 게이트 역할을 하지 못했다.

**해결:**

| 테스트 유형 | TIKETI | URR |
|------------|--------|-----|
| 백엔드 단위 테스트 | **0개** | **19개** (JUnit 5 + Mockito) |
| 통합 테스트 | 없음 | `@Tag("integration")` 분리 |
| 프론트엔드 테스트 | CRA 기본 파일만 | **Vitest + Testing Library + Playwright E2E** |
| 부하 테스트 | 없음 | **k6** (browse, booking, queue-rush, mixed 4개 시나리오) |
| 카오스 테스트 | 없음 | **k6** (service-failure, network-latency, redis-failure) |

**작성한 테스트 목록 (19개):**
- auth-service: `AuthServiceTest`, `AuthIntegrationTest`
- catalog-service: `AdminServiceTest`, `AdminDashboardServiceTest`, `ArtistServiceTest`, `EventReadServiceTest`
- community-service: `NewsServiceTest`
- gateway-service: `RateLimitFilterTest`, `VwrEntryTokenFilterTest`
- payment-service: `PaymentServiceTest`
- queue-service: `QueueServiceTest`, `SqsPublisherTest`
- stats-service: `StatsQueryServiceTest`, `StatsWriteServiceTest`
- ticket-service: `ReservationServiceTest`, `ReservationPaymentHandlerTest`, `MembershipServiceTest`, `TransferServiceTest`, `ReservationIntegrationTest`

#### 단계 4: 인프라 현대화

**문제:**
TIKETI는 Terraform이 없어 인프라를 수동 관리했고, K8s 매니페스트는 `00-namespace.yaml` ~ `14-stats-service.yaml`로 환경별 분리 없이 flat하게 관리되었다.

**해결:**

| 항목 | TIKETI | URR |
|------|--------|-----|
| IaC | 없음 | **Terraform 20개 모듈** (VPC, EKS, RDS, ElastiCache, MSK, CloudFront, WAF, Route53, ECR 등) |
| K8s 매니페스트 | 번호 기반 flat 15개 | **Kustomize base/overlay** (kind, dev, staging, prod) |
| 환경 분리 | Kind 로컬만 | **Kind / Dev / Staging / Prod** (4단계) |
| 배포 전략 | `kubectl apply` | **Argo Rollouts Blue-Green** (gateway, ticket, payment, queue) |
| 노드 스케일링 | 없음 | **Karpenter** (30초 내 노드 프로비저닝) |
| Pod 스케일링 | 없음 | **HPA** (CPU 70%) + PDB |
| DB | 단일 PostgreSQL | **RDS Multi-AZ + Read Replica** + DataSource 라우팅 |
| 캐시 | DragonflyDB 로컬 | **ElastiCache Redis 7.1** (Primary + Replica, Auth Token) |
| 메시지 브로커 | Redis Pub/Sub | **MSK (Kafka 3.6.0)** TLS + IAM 인증 |
| CDN | 없음 | **CloudFront** (5개 캐시 동작 + Lambda@Edge) |

#### 단계 5: CI/CD 고도화

**문제:**
TIKETI의 5개 CI/CD 워크플로우는 각 250줄 이상으로 중복이 심했고, PR 검증, 롤백, 보안 스캔이 없었다.

**해결:**

| 항목 | TIKETI | URR |
|------|--------|-----|
| 워크플로우 수 | 5개 (개별 작성) | **14개** (재사용 템플릿 기반) |
| 재사용 템플릿 | 없음 | **`reusable-spring-ci-cd.yml`** (1개로 8개 서비스 커버) |
| PR 검증 | 없음 | **`pr-validation.yml`** (변경 서비스만 path-based 매트릭스 테스트) |
| 보안 스캔 | 없음 | **Trivy** (CRITICAL/HIGH 차단) |
| 인증 | AWS 장기 자격증명 | **GitHub OIDC → AWS IAM** (장기 키 불필요) |
| 롤백 | 없음 | **`rollback.yml`** |
| 알림 | 없음 | **Discord webhook** |

#### 단계 6: 프론트엔드 현대화

**문제:**
TIKETI는 React CRA + JavaScript 기반 SPA로 SSR 불가, 타입 안전성 부재, 페이지별 CSS 파일(Home.css, EventDetail.css 등) 충돌 위험이 있었다.

**해결:**

| 항목 | TIKETI | URR |
|------|--------|-----|
| 프레임워크 | React 18.2 (CRA) | **Next.js 16.1.6** (App Router) |
| 언어 | JavaScript | **TypeScript 5.9.3** |
| 스타일링 | 페이지별 CSS (Home.css 등) | **Tailwind CSS v4** |
| 서버 상태 | 직접 fetch | **TanStack Query v5** |
| 결제 | Toss Payments SDK | **Toss Payments SDK** (유지) |
| 테스트 | 없음 | **Vitest + Playwright** |
| SEO | 불가 (SPA) | **SSR/SSG 지원** |

---

## 4. 성장 후기

### 배운 점

**1) "공유 라이브러리"가 진짜 DRY를 만든다**

TIKETI에서 인증 미들웨어(`auth.js`)가 4곳에 복사-붙여넣기되어 있었다. 하나를 수정하면 나머지 3곳도 똑같이 수정해야 했고, 실수로 누락하면 서비스 간 인증 동작이 달라지는 위험이 있었다. URR에서 `urr-common`이라는 공유 라이브러리를 만들어 `JwtTokenParser`, `GlobalExceptionHandler`, `InternalTokenValidator` 등을 한 곳에 모았다. 이제 한 곳을 수정하면 6개 서비스에 자동 반영된다. 진정한 "One Source of Truth"가 무엇인지 체감했다.

**2) 보안은 "계층"으로 쌓는 것이다**

TIKETI에서 각 서비스가 JWT를 개별 검증하면서 JWT 시크릿이 4곳에 분산되어 있었다. URR에서는 Gateway 한 곳에서만 JWT를 검증하고 X-User-* 헤더로 전파하는 패턴으로 바꿨다. 여기에 Rate Limiting → VWR 대기열 → Entry Token → 좌석 잠금(Redis Lua + DB FOR UPDATE + Fencing Token)까지 계층적 보안을 적용하면서, "하나가 뚫려도 다음 계층이 막는" 방어 심층(Defense in Depth) 원칙을 실제로 구현해봤다.

**3) 테스트 없는 CI/CD는 빌드 확인기에 불과하다**

TIKETI에서 CI/CD 파이프라인이 빌드 성공만 확인했다. URR에서 19개 단위 테스트를 작성하고 PR 검증 워크플로우를 만들면서, CI가 실제로 "이 코드를 머지해도 되는가?"를 판단하는 품질 게이트가 되었다. `@Tag("integration")` 분리로 인프라 의존 테스트가 단위 테스트를 방해하지 않도록 한 것도 중요한 배움이었다.

**4) Infrastructure as Code는 "재현 가능성"이다**

TIKETI에서 AWS 리소스를 콘솔로 관리하면 "지금 프로덕션에 뭐가 있는지" 정확히 아는 사람이 없었다. Terraform 20개 모듈로 코드화하면서 `terraform plan`으로 변경사항을 사전 검토하고, Staging과 Prod 환경 차이를 변수 하나(`environment = "staging"`)로 관리할 수 있게 되었다.

**5) Spring Boot 생태계의 깊이**

단순한 언어 전환이 아니었다. DI 컨테이너, `@ConditionalOnProperty` 조건부 빈, `@Transactional(readOnly = true)` 기반 Read Replica 라우팅, Resilience4j Circuit Breaker, Spring Cloud Gateway 필터 체인 등 엔터프라이즈 생태계 전체를 활용하는 법을 배웠다.

### 어려웠던 점

**1) Mockito varargs 매칭의 함정**

`JdbcTemplate.queryForList(String, Object...)`처럼 가변 인자(varargs)를 받는 메서드를 Mockito로 모킹할 때, `when().thenReturn()`과 `any()` 매처 조합이 작동하지 않았다. `doReturn().when()` 패턴과 정확한 `eq()` 값 매칭으로 해결했는데, 이 문제로 하루를 소비했다.

**2) Gradle Composite Build + scanBasePackages 누락 버그**

`urr-common`을 Gradle Composite Build로 연결한 후, `payment-service`의 `@SpringBootApplication`에 `scanBasePackages`를 빠뜨려 urr-common의 빈을 찾지 못하는 Critical 버그가 발생했다. 빌드는 성공하지만 런타임에 `NoSuchBeanDefinitionException`이 발생하는 유형이라 디버깅이 어려웠다.

**3) CI 환경에서의 통합 테스트 분리**

`@SpringBootTest` 컨텍스트 로딩 테스트가 Redis/Kafka 없는 CI 환경에서 실패했다. `@Tag("integration")`으로 분리하고 `build.gradle`에 `excludeTags 'integration'`을 설정해서 해결했는데, 각 서비스마다 이 설정을 확인하고 적용하는 과정이 번거로웠다.

**4) 186개 파일 일괄 리네이밍(tiketi → urr)의 위험**

Java 패키지, K8s 매니페스트, Terraform, Docker, CI/CD, 프론트엔드 쿠키명까지 전체 코드베이스를 일괄 변경했다. 하나라도 누락되면 빌드나 배포가 깨지는 위험이 있었고, 실제로 CI에서 6개 테스트가 실패하여 하나씩 추적해서 수정했다.

### 앞으로 적용할 점

**1) 서비스 간 mTLS 도입**

현재 `INTERNAL_API_TOKEN` + NetworkPolicy로 서비스 간 통신을 보호하고 있다. 서비스가 30개 이상으로 성장하면 Istio 또는 Linkerd를 도입하여 자동 mTLS를 적용하고, Zero-Trust 네트워크를 구현할 계획이다.

**2) Contract Testing**

서비스 간 API 계약을 Pact 기반으로 검증하여, 한 서비스의 API 변경이 다른 서비스를 깨뜨리지 않도록 보장하는 체계를 구축할 것이다.

**3) 카오스 엔지니어링 자동화**

현재 k6 기반 카오스 테스트 시나리오가 존재하지만 수동 실행이다. 정기적으로 자동 실행되는 체계를 구축하여 장애 복원력을 지속적으로 검증할 것이다.

**4) Adaptive Sampling 분산 트레이싱**

현재 Zipkin + Micrometer로 프로덕션 샘플링률 10%를 적용하고 있다. 에러 발생 시 100%로 동적 조절하는 adaptive sampling을 도입하여, 장애 상황에서 더 정확한 트레이스를 확보할 것이다.

**5) DB 마이그레이션 CI 자동화**

Flyway를 이미 사용 중이지만, CI에서 마이그레이션 드라이런을 자동 실행하는 파이프라인을 추가하여, 프로덕션 배포 전 스키마 변경의 안전성을 사전 검증할 것이다.
