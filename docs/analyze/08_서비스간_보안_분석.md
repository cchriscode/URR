# URR 서비스 간 보안 분석 — mTLS 부재와 현재 보호 전략

> 분석 대상: 서비스 간 인증/암호화 구조
> 분석 일자: 2026-02-17

---

## 1. 결론 요약

**URR은 mTLS를 사용하지 않는다.** 서비스 간 통신은 클러스터 내부 평문 HTTP이며, 3가지 보호 레이어로 보안을 확보한다.

| 보호 레이어 | 방식 | 역할 |
|-------------|------|------|
| L3/L4 네트워크 격리 | K8s NetworkPolicy (default-deny + 허용 목록) | 허가된 Pod만 통신 가능 |
| L7 토큰 인증 | `INTERNAL_API_TOKEN` (timing-safe 비교) | `/internal/*` 엔드포인트 접근 제어 |
| 서비스 노출 제한 | ClusterIP (클러스터 외부 접근 차단) | 외부에서 직접 서비스 호출 불가 |

---

## 2. 현재 통신 구조

### 2.1 트래픽 흐름

```
[외부 트래픽]
인터넷 → CloudFront (TLS) → ALB (TLS) → Gateway Pod (평문 HTTP)
                                              ↓
[클러스터 내부 — 전부 평문 HTTP]
Gateway ──HTTP──→ auth-service:3005
Gateway ──HTTP──→ ticket-service:3002
Gateway ──HTTP──→ payment-service:3003
Gateway ──HTTP──→ catalog-service:3009
Gateway ──HTTP──→ queue-service:3007
Gateway ──HTTP──→ stats-service:3004
Gateway ──HTTP──→ community-service:3008

payment-service ──HTTP──→ ticket-service:3002/internal/*
ticket-service  ──HTTP──→ payment-service:3003/internal/*
catalog-service ──HTTP──→ auth-service:3005/internal/*
queue-service   ──HTTP──→ catalog-service:3009/internal/*

[데이터 레이어 — TLS 사용]
서비스 ──TLS──→ RDS PostgreSQL (5432)       ← AWS 관리형 TLS
서비스 ──TLS──→ ElastiCache Redis (6379)    ← transit_encryption_enabled
서비스 ──TLS──→ MSK Kafka (9094)            ← client_broker = "TLS"
```

**핵심**: 외부→클러스터 구간과 서비스→데이터 레이어 구간은 TLS 암호화되지만, **서비스 간 통신(Pod↔Pod)은 평문 HTTP**이다.

### 2.2 외부 통신 TLS 현황

| 구간 | 암호화 | 출처 |
|------|--------|------|
| Client → CloudFront | TLS 1.3 | `terraform/modules/cloudfront/main.tf` (ACM 인증서) |
| CloudFront → ALB | HTTPS | `cloudfront/main.tf:91-106` (HTTPS only origin) |
| ALB → EKS Pod | **평문 HTTP** | `alb/main.tf:119-150` (HTTP target group) |
| Pod → RDS | TLS | `rds/main.tf:59` (storage_encrypted) |
| Pod → Redis | **TLS** | `elasticache/main.tf:113` (transit_encryption_enabled) |
| Pod → Kafka | **TLS** | `msk/main.tf:163` (client_broker = "TLS") |
| Pod ↔ Pod | **평문 HTTP** | ClusterIP 서비스, 암호화 없음 |

Redis TLS 설정 확인:
- Terraform: `terraform/modules/elasticache/main.tf:113` — `transit_encryption_enabled = true`
- Spring Boot: `ticket-service/application.yml:123-124` — `ssl.enabled: true` (prod 프로필)

---

## 3. 보호 레이어 1: NetworkPolicy (L3/L4 격리)

### 3.1 Default Deny

모든 Pod 간 통신을 기본 차단한 후, 필요한 경로만 허용하는 화이트리스트 방식이다.

**출처**: `k8s/spring/base/network-policies.yaml:1-9`

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}         # 모든 Pod에 적용
  policyTypes:
    - Ingress             # 들어오는 트래픽 전부 차단
    - Egress              # 나가는 트래픽 전부 차단
```

### 3.2 Ingress 허용 규칙 (누가 누구에게 접근 가능한가)

**출처**: `k8s/spring/base/network-policies.yaml:11-177`

| 대상 서비스 | 허용된 호출자 | 포트 | 라인 |
|-------------|-------------|------|------|
| gateway-service | **모든 Pod** (ALB 트래픽 수신) | 3001 | 11-23 |
| frontend | **모든 Pod** (ALB 트래픽 수신) | 3000 | 24-37 |
| auth-service | gateway, catalog | 3005 | 39-59 |
| ticket-service | gateway, payment, catalog | 3002 | 61-83 |
| catalog-service | gateway, queue | 3009 | 85-104 |
| payment-service | gateway만 | 3003 | 106-122 |
| stats-service | gateway만 | 3004 | 124-140 |
| queue-service | gateway만 | 3007 | 142-158 |
| community-service | gateway만 | 3008 | 160-176 |

**시각화:**

```
                        ┌──────────────────────────────────────┐
                        │         K8s 클러스터 내부              │
                        │                                      │
  ALB ──→ gateway-service ──→ auth-service                     │
          (누구나 접근)  │ ──→ ticket-service ←── payment-service│
                        │ ──→ catalog-service ←── queue-service │
                        │ ──→ payment-service                  │
                        │ ──→ stats-service                    │
                        │ ──→ queue-service                    │
                        │ ──→ community-service                │
                        │                                      │
                        │  ※ 위 화살표 외의 통신은 전부 차단     │
                        └──────────────────────────────────────┘
```

### 3.3 Egress 허용 규칙 (누가 어디로 나갈 수 있는가)

**출처**: `k8s/spring/base/network-policies.yaml:178-375`

| 서비스 | 허용된 목적지 | 라인 |
|--------|-------------|------|
| auth-service | DNS(53)만 | 179-197 |
| ticket-service | payment-service, data tier, DNS | 198-223 |
| payment-service | ticket-service, data tier, DNS | 225-250 |
| queue-service | catalog-service, data tier, DNS | 252-277 |
| stats-service | data tier, DNS | 279-300 |
| catalog-service | auth-service, data tier, DNS | 302-327 |
| community-service | data tier, DNS | 329-350 |
| gateway-service | `tier: backend` 전체, kube-dns | 352-375 |

### 3.4 Prod 추가 Egress (외부 데이터 레이어)

**출처**: `k8s/spring/overlays/prod/network-policy-egress.yaml:1-34`

Prod 환경에서는 AWS 관리형 서비스(RDS, ElastiCache, MSK)가 VPC 내부 서브넷에 있으므로, IP CIDR 기반으로 egress를 허용한다.

```yaml
egress:
  - to:
      - ipBlock:
          cidr: 10.0.20.0/23  # DB 서브넷 → PostgreSQL
    ports:
      - port: 5432
  - to:
      - ipBlock:
          cidr: 10.0.30.0/23  # Cache 서브넷 → Redis
    ports:
      - port: 6379
  - to:
      - ipBlock:
          cidr: 10.0.40.0/23  # Streaming 서브넷 → Kafka
    ports:
      - port: 9092
      - port: 9094
```

---

## 4. 보호 레이어 2: INTERNAL_API_TOKEN (L7 인증)

### 4.1 개요

서비스 간 `/internal/*` API 호출 시, 모든 서비스가 동일한 공유 토큰(`INTERNAL_API_TOKEN`)을 사용하여 인증한다.

**토큰 저장 위치:**
- Kind: `k8s/spring/overlays/kind/secrets.env:31` — `INTERNAL_API_TOKEN=dev-internal-token-change-me`
- Staging: `k8s/spring/overlays/staging/secrets.env.example:14` — `INTERNAL_API_TOKEN=<change-me>`
- Prod: `k8s/spring/overlays/prod/secrets.env.example:12` — `INTERNAL_API_TOKEN=<strong-random-token>`

모든 서비스가 동일한 K8s Secret에서 같은 토큰을 주입받는다.

### 4.2 토큰 검증 구현

4개 서비스에 동일한 `InternalTokenValidator` 패턴이 적용되어 있다.

**출처**: `auth-service/src/main/java/guru/urr/authservice/security/InternalTokenValidator.java:1-33`

```java
@Component
public class InternalTokenValidator {

    private final String internalToken;

    public InternalTokenValidator(@Value("${INTERNAL_API_TOKEN}") String internalToken) {
        this.internalToken = internalToken;
    }

    public boolean isValid(String authorization, String xInternalToken) {
        // 1순위: x-internal-token 헤더
        if (xInternalToken != null && !xInternalToken.isBlank()) {
            return timingSafeEquals(internalToken, xInternalToken);
        }
        // 2순위: Authorization: Bearer 헤더
        if (authorization == null || !authorization.startsWith("Bearer ")) {
            return false;
        }
        String token = authorization.substring(7);
        return timingSafeEquals(internalToken, token);
    }

    // Timing-safe 비교 (타이밍 공격 방지)
    private static boolean timingSafeEquals(String a, String b) {
        return MessageDigest.isEqual(
                a.getBytes(StandardCharsets.UTF_8),
                b.getBytes(StandardCharsets.UTF_8));
    }
}
```

**핵심 보안 포인트:**
- `MessageDigest.isEqual()` 사용 → **타이밍 공격 방지** (문자열 비교 시간이 일정)
- 두 가지 헤더 지원: `x-internal-token` (직접) 또는 `Authorization: Bearer` (표준)

### 4.3 필터 적용 (Internal API 보호)

**출처**: `auth-service/src/main/java/guru/urr/authservice/security/InternalApiAuthFilter.java:1-53`

```java
@Component
public class InternalApiAuthFilter extends OncePerRequestFilter {

    @Override
    protected void doFilterInternal(HttpServletRequest request, ...) {
        String token = request.getHeader("x-internal-token");
        if (token == null || token.isBlank()) {
            // Authorization: Bearer 폴백
            String authHeader = request.getHeader("Authorization");
            if (authHeader != null && authHeader.startsWith("Bearer ")) {
                token = authHeader.substring(7);
            }
        }

        // Timing-safe 비교 실패 시 403 Forbidden
        if (token == null || !MessageDigest.isEqual(...)) {
            response.setStatus(403);
            response.getWriter().write("{\"error\":\"Forbidden\"}");
            return;
        }

        filterChain.doFilter(request, response);
    }

    @Override
    protected boolean shouldNotFilter(HttpServletRequest request) {
        return !request.getRequestURI().startsWith("/internal/");  // /internal/* 경로만 필터링
    }
}
```

### 4.4 적용 서비스 및 내부 API 목록

| 서비스 | InternalTokenValidator 파일 | 보호되는 엔드포인트 |
|--------|---------------------------|-------------------|
| auth-service | `authservice/security/InternalTokenValidator.java` | `GET /internal/users/{id}`, `POST /internal/users/batch` |
| ticket-service | `ticketservice/shared/security/InternalTokenValidator.java` | `GET /internal/reservations/{id}/validate`, `POST /internal/reservations/{id}/confirm`, `GET /internal/transfers/{id}/validate`, `POST /internal/transfers/{id}/complete`, `GET /internal/memberships/{id}/validate`, `POST /internal/memberships/{id}/activate` |
| payment-service | `paymentservice/security/InternalTokenValidator.java` | `POST /internal/payments/...` |
| catalog-service | `catalogservice/shared/security/InternalTokenValidator.java` | `GET /internal/events/...` |

### 4.5 호출 측 구현 예시

**출처**: `payment-service/src/main/java/guru/urr/paymentservice/client/TicketInternalClient.java`

```java
// Payment → Ticket 내부 호출 시 토큰 첨부
HttpHeaders headers = new HttpHeaders();
headers.set("Authorization", "Bearer " + internalApiToken);

restTemplate.exchange(
    ticketServiceUrl + "/internal/reservations/" + id + "/validate?userId=" + userId,
    HttpMethod.GET,
    new HttpEntity<>(headers),
    Map.class
);
```

---

## 5. 보호 레이어 3: ClusterIP 서비스 타입

### 5.1 구조

모든 백엔드 서비스는 `ClusterIP` 타입으로 배포되어 클러스터 외부에서 직접 접근이 불가능하다.

**출처**: `k8s/spring/base/gateway-service/service.yaml:1-13`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway-service
spec:
  type: ClusterIP          # 클러스터 내부에서만 접근 가능
  selector:
    app: gateway-service
  ports:
    - port: 3001
      targetPort: 3001
```

외부 트래픽은 반드시 **ALB → Gateway Service → 내부 서비스** 경로를 거쳐야 한다. 개별 서비스(auth, ticket 등)에 직접 접근하는 것은 물리적으로 불가능하다.

---

## 6. mTLS와의 비교

### 6.1 기능별 비교

| 보안 기능 | mTLS (Istio) | URR 현재 (NetworkPolicy + Token) |
|-----------|-------------|----------------------------------|
| **통신 암호화** | 자동 TLS (Pod↔Pod) | 없음 (클러스터 내부 평문 HTTP) |
| **상호 인증** | 인증서 기반 (서비스별 고유 ID) | 공유 토큰 1개 (전 서비스 동일) |
| **인증서 관리** | 자동 발급/갱신/교체 (istiod) | 불필요 (토큰 기반) |
| **접근 제어 세분화** | AuthorizationPolicy (서비스×경로) | NetworkPolicy (Pod×포트) + 코드 검증 |
| **토큰 탈취 위험** | 없음 (인증서는 Pod에 바인딩) | 토큰 유출 시 모든 내부 API 접근 가능 |
| **네트워크 스니핑** | 불가 (TLS 암호화) | 가능 (클러스터 내부 평문) |
| **운영 복잡도** | 높음 (istiod + sidecar) | 낮음 (YAML + Java 코드) |
| **리소스 비용** | Pod당 Envoy sidecar (~50MB) | 없음 |
| **latency 추가** | ~1-3ms per hop | 없음 |

### 6.2 공격 시나리오별 비교

| 공격 시나리오 | mTLS | URR 현재 | 위험도 |
|--------------|------|----------|--------|
| 클러스터 외부에서 서비스 직접 호출 | 차단 | **차단** (ClusterIP) | 낮음 |
| 허가되지 않은 Pod에서 서비스 호출 | 차단 (인증서 없음) | **차단** (NetworkPolicy) | 낮음 |
| 허가된 Pod에서 잘못된 내부 API 호출 | 차단 (AuthorizationPolicy) | **차단** (토큰 검증) | 낮음 |
| 클러스터 내부 네트워크 스니핑 | **차단** (TLS) | 노출 (평문) | 중간 |
| INTERNAL_API_TOKEN 유출 | 해당 없음 | 모든 내부 API 접근 가능 | 중간 |
| 컨테이너 탈출 (container escape) | 제한적 방어 | 제한적 방어 | 높음 |

### 6.3 현실적 위험 평가

**클러스터 내부 스니핑 위험:**
- EKS 워커 노드는 AWS VPC 내부에 있고, 노드 간 통신은 VPC 네트워크를 통해 이루어진다
- VPC 내부 트래픽은 AWS 인프라에서 격리되어 있어 외부 스니핑이 사실상 불가능
- 위험은 **노드가 침해된 경우**에만 현실적 → 이 경우 mTLS도 완전한 방어 불가

**INTERNAL_API_TOKEN 유출 위험:**
- 토큰은 K8s Secret으로 관리 → Secret 접근 권한이 있는 사람만 열람 가능
- 모든 서비스가 같은 토큰 → 하나의 서비스 컨테이너 침해 시 다른 서비스 내부 API 접근 가능
- **완화 방안**: 서비스별 다른 토큰 사용 (현재 미적용)

---

## 7. AWS 배포 시 추가되는 TLS 구간

현재 Terraform 설정에서 데이터 레이어는 이미 TLS가 적용되어 있다.

### 7.1 이미 TLS가 적용된 구간

| 구간 | TLS 설정 | 출처 |
|------|---------|------|
| Client → CloudFront | TLS 1.3 (ACM 인증서) | `cloudfront/main.tf` |
| CloudFront → ALB | HTTPS origin | `cloudfront/main.tf:94` (`https-only`) |
| Pod → RDS | TLS (RDS 기본 강제) | `rds/main.tf:59` |
| Pod → ElastiCache | TLS | `elasticache/main.tf:113` (`transit_encryption_enabled = true`) |
| Pod → MSK | TLS | `msk/main.tf:163` (`client_broker = "TLS"`) |
| Spring Boot → Redis | SSL | `ticket-service/application.yml:123-124` (`ssl.enabled: true`, prod 프로필) |

### 7.2 TLS가 없는 구간

| 구간 | 현재 상태 | 위험도 | 완화 방안 |
|------|----------|--------|-----------|
| ALB → Gateway Pod | 평문 HTTP | 낮음 (VPC 내부) | ALB → HTTPS target group 전환 |
| Gateway → 내부 서비스 | 평문 HTTP | 낮음 (같은 노드/VPC) | Istio mTLS 또는 Spring TLS |
| 서비스 ↔ 서비스 (internal) | 평문 HTTP | 중간 | Istio mTLS 권장 |

---

## 8. 평가

### 8.1 현재 구조의 강점

1. **Default-Deny NetworkPolicy**: 모든 통신을 기본 차단 후 화이트리스트로 허용 — 최소 권한 원칙 적용 (`network-policies.yaml:1-9`)
2. **서비스별 세분화된 Ingress/Egress 규칙**: 8개 서비스 각각에 대해 허용된 호출자와 목적지를 명시 (총 17개 NetworkPolicy)
3. **Timing-safe 토큰 비교**: `MessageDigest.isEqual()` 사용으로 타이밍 공격 방지 (`InternalTokenValidator.java:28-31`)
4. **Prod Egress CIDR 제한**: AWS VPC 서브넷 단위로 외부 접근을 제한 (`network-policy-egress.yaml`)
5. **데이터 레이어 TLS 완비**: RDS, Redis, Kafka 모두 전송 중 암호화 적용
6. **운영 단순성**: Istio 없이도 실용적인 보안 수준 확보

### 8.2 미흡한 점

1. **Pod 간 평문 HTTP**: 클러스터 내부 서비스 간 통신이 암호화되지 않음
2. **단일 공유 토큰**: `INTERNAL_API_TOKEN` 하나로 모든 서비스 인증 → 토큰 유출 시 전체 내부 API 노출
3. **서비스 ID 부재**: 호출자가 "어떤 서비스"인지 토큰만으로 구분 불가 (gateway든 payment든 같은 토큰)
4. **ALB → Pod 구간 평문**: CloudFront→ALB는 HTTPS지만 ALB→Pod는 HTTP (VPC 내부이므로 위험도 낮음)

### 8.3 향후 보완 방안

| 방안 | 복잡도 | 효과 | 적합 시점 |
|------|--------|------|-----------|
| 서비스별 별도 `INTERNAL_API_TOKEN` | 낮음 | 토큰 유출 피해 범위 축소 | 즉시 적용 가능 |
| ALB Target Group HTTPS 전환 | 낮음 | ALB→Pod TLS 확보 | 즉시 적용 가능 |
| AWS App Mesh (Envoy 기반) | 중간 | AWS 네이티브 mTLS | 서비스 15개+ |
| Linkerd (경량 서비스 메시) | 중간 | 자동 mTLS, 낮은 오버헤드 | 서비스 15-30개 |
| Istio (풀 서비스 메시) | 높음 | mTLS + 관찰성 + 정책 관리 | 서비스 30개+ 또는 규제 요구 |

---

## 9. 토큰 아키텍처와 mTLS의 관계

### 9.1 URR의 3개 JWT 토큰 흐름

URR은 사용자 여정 단계별로 3개의 JWT를 발급한다.

```
[1단계] 로그인
    auth-service 발급 → Auth JWT
    ├── access_token  (30분, Authorization 헤더)
    └── refresh_token (7일, HttpOnly 쿠키)
    용도: 사용자 인증 (모든 API 호출에 사용)
    출처: auth-service/security/JwtService.java:104-116

        ↓ 이벤트 상세 → "예매하기" 클릭

[2단계] VWR Tier 1 대기열 통과
    VWR Lambda 발급 → VWR Token
    ├── urr-vwr-token 쿠키 (10분 TTL)
    ├── claims: { sub: eventId, uid: userId, tier: 1 }
    └── 서명: HMAC-SHA256 (VWR_TOKEN_SECRET)
    용도: "VWR 대기열을 통과했다"는 증거, Tier 2 큐 진입 시 우선순위 힌트
    출처: lambda/vwr-api/lib/token.js:11-27

        ↓ Tier 2 큐 대기 → 입장 허가

[3단계] Queue Tier 2 대기열 통과
    queue-service 발급 → Entry Token
    ├── urr-entry-token 쿠키 (10분 TTL)
    ├── claims: { sub: eventId, uid: userId }
    └── 서명: HMAC-SHA256 (QUEUE_ENTRY_TOKEN_SECRET)
    용도: 좌석 선택/예매 API 접근 허가 (Lambda@Edge가 검증)
    출처: queue-service/service/QueueService.java:224-236

        ↓ 좌석 선택 → 결제 → 예매 완료
```

### 9.2 각 토큰의 검증 지점

| 토큰 | 발급자 | 검증자 | 검증 시점 |
|------|--------|--------|-----------|
| Auth JWT (access) | auth-service | Gateway 필터 체인 (모든 요청) | 매 API 호출 |
| Auth JWT (refresh) | auth-service | auth-service (갱신 요청 시) | access 만료 시 |
| VWR Token | VWR Lambda | queue-service (vwrPosition 파라미터로 전달) | Tier 2 큐 진입 시 |
| Entry Token | queue-service | Lambda@Edge (`edge-queue-check/index.js`) | `/api/seats/*`, `/api/reservations/*` 접근 시 |

### 9.3 토큰 탈취 경로와 mTLS의 관계

**mTLS는 토큰 탈취 방지 수단이 아니다.** 각 탈취 경로별 실제 방어 수단은 다음과 같다.

| 탈취 경로 | 대상 토큰 | mTLS로 방어? | 실제 방어 수단 |
|-----------|----------|-------------|---------------|
| 브라우저 XSS → 쿠키 탈취 | VWR Token, Entry Token | **아니오** | HttpOnly 쿠키, CSP nonce (`middleware.ts:10-18`) |
| 브라우저 JS → access_token 탈취 | Auth JWT | **아니오** | 쿠키 저장 (HttpOnly/Secure/SameSite) |
| 클라이언트 네트워크 스니핑 (Wi-Fi 등) | 모든 토큰 | **아니오** | HTTPS (CloudFront TLS 1.3) |
| 클러스터 내부 Pod 간 스니핑 | INTERNAL_API_TOKEN | **예** | 현재: NetworkPolicy + ClusterIP |
| 서비스 위장 (Pod 사칭) | INTERNAL_API_TOKEN | **예** (인증서로 신원 확인) | 현재: NetworkPolicy |
| K8s Secret 접근 | INTERNAL_API_TOKEN | **아니오** | RBAC, Secret 암호화 |

### 9.4 mTLS가 실제로 보호하는 것

mTLS의 핵심 가치는 토큰 탈취 방지가 아니라 **서비스 간 통신 보안**이다.

```
mTLS가 해결하는 문제:
├── 1. Pod 간 통신 암호화
│      현재: Gateway → ticket-service:3002 (평문 HTTP)
│      mTLS: Gateway [Envoy] ──TLS── [Envoy] ticket-service (자동 암호화)
│
├── 2. 서비스 신원 증명
│      현재: "이 요청에 INTERNAL_API_TOKEN이 있으니 내부 서비스다"
│      mTLS: "이 요청은 spiffe://cluster.local/ns/urr-spring/sa/payment-service 인증서를 가진 Pod에서 왔다"
│
└── 3. INTERNAL_API_TOKEN 제거
       현재: 토큰 1개 유출 → 모든 내부 API 접근 가능
       mTLS: 토큰 자체가 불필요 → 인증서 기반 상호 인증

mTLS가 해결하지 못하는 문제:
├── 사용자 토큰(Auth/VWR/Entry) 탈취 → 클라이언트 측 문제
├── 브라우저 XSS/CSRF 공격 → 프론트엔드 보안 문제
└── K8s Secret 유출 → RBAC/Secret 관리 문제
```

### 9.5 현실적 위험 평가

| 위험 | 발생 조건 | 현재 방어 | mTLS 추가 시 | 판정 |
|------|----------|----------|-------------|------|
| 클러스터 내부 스니핑 | 노드 침해 필요 (AWS VPC 내부) | NetworkPolicy + VPC 격리 | TLS 암호화 추가 | 위험 낮음 |
| INTERNAL_API_TOKEN 유출 | Secret 접근 또는 Pod 침해 | Timing-safe 비교 + NetworkPolicy | 토큰 자체 제거 | 위험 중간 |
| 서비스 위장 공격 | 같은 네임스페이스에 악성 Pod 배포 | NetworkPolicy 라벨 기반 차단 | 인증서 기반 검증 | 위험 낮음 |
| 사용자 JWT 탈취 | XSS, 네트워크 스니핑 | HttpOnly + HTTPS + CSP | **효과 없음** | mTLS 무관 |

**결론**: 현재 8개 서비스, 단일 EKS 클러스터 규모에서 mTLS 도입은 보안 이득 대비 운영 복잡도가 높다. AWS VPC 내부 + NetworkPolicy + INTERNAL_API_TOKEN 조합이 실용적 수준의 보안을 제공하며, 서비스 규모가 30개 이상으로 성장하거나 규제 요구사항(PCI-DSS 등)이 생기는 시점에 Istio 또는 경량 대안을 도입하면 된다.

---

## 10. mTLS 도입 시 서버 비용 영향

### 10.1 Istio Sidecar (Envoy Proxy) 리소스 오버헤드

mTLS를 사용하려면 모든 Pod에 Envoy sidecar 컨테이너가 추가된다. Pod당 추가 리소스:

| 리소스 | Pod당 추가분 | 비고 |
|--------|-------------|------|
| Memory | **+40~100MB** | Envoy 기본 메모리 footprint |
| CPU | **+10~50m** | TLS handshake + 암호화/복호화 처리 |

URR 8개 서비스 기준 총 추가량:

| 환경 | replica 수 | 추가 Memory | 추가 CPU |
|------|-----------|-------------|----------|
| Dev/Kind | 8 × 1 = 8 Pod | +320MB ~ 800MB | +80m ~ 400m |
| Prod | 8 × 3 = 24 Pod | **+960MB ~ 2.4GB** | **+240m ~ 1.2 CPU** |

### 10.2 Istio Control Plane 리소스

Istio 자체 컴포넌트도 별도 리소스를 소비한다.

| 컴포넌트 | 역할 | Memory | CPU |
|----------|------|--------|-----|
| **istiod** (필수) | 인증서 발급, 설정 배포, xDS API | 500MB ~ 1GB | 500m |
| Kiali (선택) | 서비스 메시 시각화 대시보드 | ~200MB | 100m |
| Jaeger (선택) | 분산 트레이싱 | 500MB+ | 200m |

istiod만 설치해도 **별도 노드 1개가 추가로 필요할 수 있다.**

### 10.3 TLS Handshake 성능 오버헤드

| 항목 | 영향 |
|------|------|
| 첫 연결 Latency | **+1~3ms** per request (TLS handshake) |
| 이후 연결 (connection reuse) | ~0.1ms (session resumption) |
| CPU 사용량 | 암호화/복호화 연산으로 증가 |
| 내부 트래픽 비례 | 서비스 간 호출이 많을수록 누적 영향 증가 |

URR의 예매 플로우: Gateway → queue → catalog → ticket → payment → ticket (최소 5 hop)
→ 각 hop에 +1~3ms → **총 +5~15ms latency 추가**

### 10.4 URR 기준 예상 비용 증가

```
현재 EKS 노드 구성:
  Karpenter 자동 스케일링: t3.medium ~ t3.xlarge
  Prod 예상 노드: 3~5대

mTLS(Istio) 도입 시 추가:
  Sidecar 메모리: ~1~2.4GB (Prod, 24 Pod)
  istiod: 500MB~1GB + 500m CPU
  → 기존 노드에 수용 불가 시 노드 1~2대 추가

예상 추가 월 비용:
  t3.medium 1대: ~$30/월
  t3.large 1대: ~$60/월
  총 추가: 월 $50~150 (노드 추가 + 데이터 전송)
```

### 10.5 비용 대비 효과 종합

| 항목 | 현재 (INTERNAL_API_TOKEN) | mTLS (Istio) |
|------|--------------------------|--------------|
| 추가 인프라 비용 | $0 | **+$50~150/월** |
| 추가 Latency | 0 | **+5~15ms** (예매 플로우 전체) |
| 보안 수준 | 공유 토큰 기반 (적정) | 인증서 기반 (강력) |
| 운영 복잡도 | 낮음 (Java 코드 + YAML) | **높음** (Istio 운영 전문성 필요) |
| 필요 전문성 | Spring Security | **+Istio, Envoy, mTLS 인증서 관리** |
| 장애 시 영향 | 개별 서비스 장애 | **istiod 장애 → 전체 통신 장애 가능** |
| 디버깅 난이도 | 평문 → tcpdump/curl로 즉시 확인 | **TLS 암호화 → 디버깅 도구 추가 필요** |

### 10.6 결론

비용보다 더 큰 문제는 **운영 복잡도**이다.

1. **비용 자체는 크지 않다**: 월 $50~150 수준으로, 대규모 서비스에서는 무시할 수 있는 금액
2. **진짜 비용은 운영 부담**: Istio 장애 시 전체 서비스 통신이 중단될 수 있고, 디버깅이 훨씬 어려움
3. **현재 규모(8개 서비스)에서는 과잉**: NetworkPolicy + INTERNAL_API_TOKEN이 비용 대비 효율적
4. **도입 적정 시점**: 서비스 30개+, 멀티 클러스터, 또는 PCI-DSS 등 규제 요구 시
