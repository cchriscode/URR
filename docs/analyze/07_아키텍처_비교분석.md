# URR 트래픽 아키텍처 비교 분석

> 분석 대상: URR 현재 구조 vs Nginx Ingress Controller vs Istio Service Mesh
> 분석 일자: 2026-02-17

---

## 1. 현재 URR 구조

```
인터넷 → Route53 → CloudFront (WAF + Lambda@Edge)
                        ↓
                    AWS ALB (L7)
                   ┌────┴────┐
                /api/*     /* (default)
                   ↓          ↓
          Gateway Service   Frontend
          (Spring Cloud     (Next.js)
           Gateway MVC)
          ┌──┬──┬──┬──┐
         Auth Ticket Pay Queue ...
         (ClusterIP Services, 직접 HTTP 호출)
```

### 핵심 컴포넌트

| 레이어 | 기술 | 출처 |
|--------|------|------|
| CDN/Edge | CloudFront + Lambda@Edge | `terraform/modules/cloudfront/main.tf` |
| WAF | AWS WAFv2 (4 rules) | `terraform/modules/waf/main.tf:25-114` |
| 인그레스 | AWS ALB (Terraform 관리) | `terraform/modules/alb/main.tf:94-110` |
| API 라우팅 | Spring Cloud Gateway MVC (16개 라우트) | `gateway-service/src/main/resources/application.yml:9-80` |
| 인증 필터 | Java Filter Chain (Rate Limit → VWR Token → JWT) | `gateway-service/src/main/java/.../filter/RateLimitFilter.java` |
| 서비스 간 통신 | HTTP + `INTERNAL_API_TOKEN` / Kafka 비동기 | `payment-service/src/main/java/.../client/TicketInternalClient.java` |
| 노드 스케일링 | Karpenter (IRSA) | `terraform/modules/eks/main.tf:351-439` |
| Pod 스케일링 | HPA (CPU 70%) | `k8s/spring/overlays/prod/hpa.yaml` |

### 트래픽 흐름 상세

1. **CloudFront** (`cloudfront/main.tf:82-222`)
   - 5개 캐시 동작: `/*`(SSR), `/api/*`(no-cache), `/_next/static/*`(1년), `/vwr/*`(5분), `/vwr-api/*`(no-cache)
   - Lambda@Edge가 `/api/*` 요청에 대해 VWR 토큰 검증 (`cloudfront/main.tf:158-162`)
   - CloudFront Function으로 VWR SPA 리라이트 (`cloudfront/main.tf:422-453`)

2. **ALB** (`alb/main.tf:189-223`)
   - HTTPS 리스너 (443): ACM 인증서, TLS 1.2/1.3
   - 경로 규칙: `/api/*` → Gateway Target Group (3001), `/*` → Frontend Target Group (3000)
   - CloudFront 전용 접근: prefix list 또는 custom header 검증 (`alb/main.tf:21-42`)

3. **Spring Cloud Gateway** (`gateway-service/application.yml:9-75`)
   - 16개 라우트: auth, payment, stats, events, tickets, seats, reservations, queue, admin, community, news, artists, memberships, transfers, image, time
   - 필터 체인: RateLimitFilter(Order 0) → VwrEntryTokenFilter(Order 1) → CookieAuthFilter → JwtAuthFilter
   - Redis 기반 Rate Limiting: AUTH(60rpm), QUEUE(120rpm), BOOKING(30rpm), GENERAL(3000rpm)

4. **서비스 간 통신**
   - 동기: HTTP REST + `INTERNAL_API_TOKEN` (timing-safe 비교, `InternalTokenValidator.java:29`)
   - 비동기: Kafka 4개 토픽 (payment-events, reservation-events, transfer-events, membership-events)
   - 장애 격리: Resilience4j Circuit Breaker (failure-rate 50%, wait 10s)

---

## 2. Nginx Ingress Controller 구조

```
인터넷 → CloudFront → AWS NLB (L4)
                          ↓
              Nginx Ingress Controller (K8s Pod)
              ┌────────┴────────┐
           /api/*            /*
              ↓                ↓
       Gateway Service     Frontend
          ┌──┬──┬──┐
         Auth Ticket Pay ...
```

### Nginx Ingress란?

Kubernetes 네이티브 인그레스 컨트롤러로, nginx를 Pod로 배포하여 클러스터 내부에서 L7 라우팅을 수행한다. `Ingress` 리소스 또는 CRD로 라우팅 규칙을 선언적으로 관리한다.

### 설정 예시

```yaml
# Ingress 리소스 (URR에 적용한다면)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: urr-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
    - hosts: [urr.io]
      secretName: urr-tls  # cert-manager가 관리
  rules:
    - host: urr.io
      http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: gateway-service
                port:
                  number: 3001
          - path: /
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 3000
```

---

## 3. Istio Service Mesh 구조

```
인터넷 → CloudFront → ALB/NLB
                          ↓
                   Istio Ingress Gateway (Envoy Pod)
                          ↓
              ┌──── VirtualService ────┐
              ↓                        ↓
     Gateway Service              Frontend
     [Envoy Sidecar]             [Envoy Sidecar]
          ↓ mTLS 자동 암호화
     ┌────┼────┐
    Auth  Ticket  Payment  Queue ...
    [각 Pod에 Envoy Sidecar 자동 주입]
    (모든 서비스 간 통신이 mTLS로 암호화)
```

### Istio란?

서비스 메시(Service Mesh) 플랫폼으로, 모든 Pod에 Envoy 프록시 사이드카를 자동 주입하여 서비스 간 통신을 제어한다. mTLS 암호화, 트래픽 관리, 관찰성을 인프라 레벨에서 제공한다.

### 설정 예시

```yaml
# VirtualService (URR에 적용한다면)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: gateway-service
spec:
  hosts: [gateway-service]
  http:
    - route:
        - destination:
            host: gateway-service
            port:
              number: 3001
      timeout: 10s
      retries:
        attempts: 3
        perTryTimeout: 3s
---
# AuthorizationPolicy (서비스 간 접근 제어)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: payment-to-ticket-only
spec:
  selector:
    matchLabels:
      app: ticket-service
  rules:
    - from:
        - source:
            principals: ["cluster.local/ns/urr-spring/sa/payment-service"]
      to:
        - operation:
            paths: ["/internal/*"]
```

---

## 4. Karpenter (URR 이미 사용 중)

URR은 Karpenter를 이미 사용하고 있다 (`terraform/modules/eks/main.tf:351-439`).

### Cluster Autoscaler vs Karpenter

| 항목 | Cluster Autoscaler (기본) | Karpenter (URR 사용) |
|------|--------------------------|---------------------|
| **스케일링 단위** | Node Group 전체 (ASG 기반) | 개별 노드 단위 |
| **인스턴스 선택** | 고정 타입 (예: t3.medium만) | 워크로드에 맞춰 **동적 선택** |
| **반응 속도** | 2-5분 (ASG 조정 대기) | **30초-2분** (직접 EC2 API 호출) |
| **비용 최적화** | 수동 Spot 설정 | **자동 Spot/On-Demand 혼합** |
| **빈 노드 정리** | 느린 scale-down (10분+) | **적극적 consolidation** |
| **Pending Pod 해결** | Node Group 내 타입만 | 최적 인스턴스 타입 자동 계산 |

### URR에서의 Karpenter 역할

- **티켓 오픈 시**: 트래픽 폭증 → 30초 내 새 노드 프로비저닝
- **이벤트 종료 후**: 유휴 노드 적극적 consolidation → 비용 절감
- **Spot 인스턴스**: 자동 활용으로 On-Demand 대비 최대 70% 절감
- **조건부 종료**: `karpenter.sh/nodepool` 태그가 있는 EC2만 종료 가능 (`eks/main.tf:421-429`)

---

## 5. 상세 비교표

### 5.1 인그레스 레이어 비교

| 항목 | AWS ALB (URR 현재) | Nginx Ingress Controller |
|------|-------------------|--------------------------|
| **실행 위치** | AWS 관리형 (클러스터 외부) | K8s Pod (클러스터 내부) |
| **설정 방식** | Terraform HCL | K8s Ingress YAML / CRD |
| **L7 라우팅** | ALB Listener Rules | nginx.conf (annotation 기반) |
| **TLS 종료** | ALB + ACM 인증서 (자동 발급/갱신) | cert-manager + Let's Encrypt (직접 관리) |
| **비용** | ALB 시간당 과금 (~$16/월) + LCU | NLB 비용 (~$16/월) + Pod 리소스 |
| **가용성** | AWS SLA 99.99% | Pod HA 직접 구성 (replica, PDB, anti-affinity) |
| **스케일링** | AWS 자동 (투명) | Pod HPA로 직접 관리 |
| **WAF 연동** | AWS WAF 네이티브 | ModSecurity (직접 규칙 관리) |
| **설정 반영** | Terraform apply (분 단위) | kubectl apply (초 단위) |
| **CloudFront 연동** | 네이티브 (prefix list, custom header) | 추가 설정 필요 |
| **Sticky Session** | ALB 네이티브 지원 | annotation으로 설정 |

### 5.2 서비스 메시 레이어 비교

| 항목 | URR 현재 (메시 없음) | Istio Service Mesh |
|------|---------------------|-------------------|
| **서비스 간 인증** | `INTERNAL_API_TOKEN` (공유 시크릿) | **mTLS** (자동 인증서 발급/갱신) |
| **서비스 간 암호화** | 평문 HTTP (클러스터 내부) | **자동 mTLS** (모든 통신 암호화) |
| **트래픽 관찰** | Prometheus 스크래핑 (수동 설정) | **자동 L7 메트릭** (latency, error rate, RPS) |
| **트래픽 제어** | Spring Gateway 필터 + Resilience4j | VirtualService (retry, timeout, circuit breaker) |
| **카나리 배포** | Argo Rollouts Blue-Green | Istio VirtualService weight splitting |
| **접근 제어** | 코드 레벨 토큰 검증 | **AuthorizationPolicy** (YAML 선언적) |
| **분산 트레이싱** | Micrometer + Zipkin (수동 설정) | **자동 trace 전파** (Envoy가 헤더 주입) |
| **리소스 비용** | 없음 | **Pod당 Envoy sidecar** (~50MB RAM, ~10m CPU) |
| **학습 곡선** | Spring 개발자 친숙 | 높음 (CRD 20+개, 디버깅 복잡) |
| **장애 범위** | 개별 서비스 격리 | istiod 장애 시 전체 메시 영향 가능 |

### 5.3 전체 조합 비교

| 기능 | URR 현재 | + Nginx | + Istio | + Nginx + Istio |
|------|----------|---------|---------|-----------------|
| 인그레스 | ALB (AWS 관리형) | Nginx Pod | ALB or Istio GW | Nginx + Istio GW |
| API 라우팅 | Spring Cloud Gateway | Spring Cloud Gateway | Gateway 또는 VirtualService | 레이어 중복 가능 |
| 서비스 간 인증 | INTERNAL_API_TOKEN | INTERNAL_API_TOKEN | **mTLS 자동** | mTLS 자동 |
| 서비스 간 암호화 | 없음 (클러스터 내부) | 없음 | **mTLS 자동** | mTLS 자동 |
| Rate Limiting | Redis Lua (Gateway) | Nginx + Gateway | Istio + Gateway | 3중 중복 가능 |
| Circuit Breaker | Resilience4j (Java) | Resilience4j | Istio EnvoyFilter + Resilience4j | 중복 |
| 관찰성 | Prometheus + Zipkin (수동) | 동일 | **자동 L7 메트릭** | 자동 메트릭 |
| WAF | AWS WAF 네이티브 | ModSecurity (직접 운영) | AWS WAF | 혼합 |
| TLS 관리 | ACM (자동) | cert-manager (직접 운영) | ACM | 혼합 |
| 운영 복잡도 | **낮음** | 중간 | **높음** | 매우 높음 |
| 추가 리소스 | 없음 | Nginx Pod (~2-3개) | Envoy sidecar × 모든 Pod + istiod | 둘 다 |
| 월 추가 비용 (추정) | 기준 | +$20-50 | +$100-200 | +$150-250 |

---

## 6. 각 방식의 장단점 정리

### 6.1 URR 현재 구조 (ALB + Spring Cloud Gateway + Karpenter)

**장점:**
1. **운영 부담 최소**: ALB, CloudFront, WAF 모두 AWS 관리형 → HA/패치/모니터링 불필요
2. **기술 스택 일관성**: Java/Spring 팀이 라우팅/필터를 코드로 제어 → 디버깅 용이
3. **네이티브 연동**: CloudFront → ALB → EKS가 AWS 서비스 간 자연스럽게 연결
4. **낮은 latency**: 추가 프록시 레이어 없음 → 서비스 호출에 사이드카 오버헤드 없음
5. **비용 효율**: 관리형 서비스 비용만 발생, 추가 Pod 리소스 불필요
6. **빠른 노드 스케일링**: Karpenter로 티켓 오픈 시 30초 내 노드 추가

**단점:**
1. **서비스 간 암호화 부재**: 클러스터 내부 통신이 평문 HTTP
2. **공유 시크릿 방식**: `INTERNAL_API_TOKEN` 하나로 모든 서비스 간 인증 → 세분화된 접근 제어 불가
3. **AWS 종속**: ALB, CloudFront, WAF 등 전부 AWS 서비스 → 멀티 클라우드 불가
4. **선언적 트래픽 관리 부재**: retry, timeout, circuit breaker가 Java 코드에 내장 → 인프라 레벨에서 변경 불가
5. **자동 관찰성 부재**: 서비스 간 메트릭 수집을 수동으로 설정해야 함

### 6.2 Nginx Ingress Controller

**장점:**
1. **K8s 네이티브**: Ingress 리소스로 라우팅을 선언적으로 관리
2. **고급 라우팅**: canary weight, header 기반 라우팅, regex path 등
3. **클라우드 독립**: AWS, GCP, Azure, 온프레미스 어디서든 동일하게 동작
4. **빠른 설정 반영**: `kubectl apply` 즉시 반영 (Terraform 대비)
5. **오픈소스**: 라이선스 비용 없음

**단점:**
1. **HA 직접 구성**: replica, PDB, anti-affinity 등을 직접 관리
2. **TLS 인증서 관리**: cert-manager 설치 + 설정 + 갱신 모니터링 필요
3. **WAF 직접 운영**: ModSecurity 규칙을 직접 작성/유지 → AWS WAF 대비 관리 부담
4. **Pod 장애 위험**: Nginx Pod 다운 시 전체 트래픽 차단 가능
5. **CloudFront 연동 비효율**: L4(NLB) 경유 시 HTTP 헤더 조작 제한

### 6.3 Istio Service Mesh

**장점:**
1. **Zero-trust 네트워크**: 모든 서비스 간 mTLS 자동 적용 (인증서 발급/갱신/교체 자동)
2. **선언적 트래픽 정책**: retry, timeout, circuit breaker, fault injection을 YAML로 관리
3. **자동 관찰성**: 모든 서비스 간 호출의 latency/error rate/throughput 자동 수집
4. **세분화된 접근 제어**: AuthorizationPolicy로 "어떤 서비스가 어떤 서비스의 어떤 경로에 접근 가능"
5. **카나리/A-B 테스트**: 트래픽 비율 분배, 헤더 기반 라우팅이 인프라 레벨

**단점:**
1. **리소스 오버헤드**: 8서비스 × 3 replicas = 24개 Envoy sidecar (각 ~50MB RAM)
2. **latency 추가**: 모든 요청이 sidecar 프록시 경유 → 요청당 ~1-3ms 추가
3. **운영 복잡도**: istiod 컨트롤 플레인 관리, CRD 20+개 학습
4. **디버깅 어려움**: 네트워크 문제 발생 시 sidecar/istiod/mTLS 레이어까지 확인 필요
5. **장애 전파 위험**: istiod 컨트롤 플레인 장애 시 전체 서비스 메시에 영향
6. **학습 곡선**: Istio 전문 지식이 운영팀에 필요

---

## 7. URR이 현재 구조를 선택한 이유

### 7.1 AWS 네이티브 = 운영 부담 최소화

ALB, CloudFront, WAF 모두 AWS 관리형 서비스로 직접 HA/패치/모니터링이 불필요하다.

| 직접 운영 시 (Nginx + Istio) | AWS 관리형 (URR) |
|------------------------------|-----------------|
| Nginx Pod 레플리카 관리 | ALB 자동 스케일링 |
| cert-manager TLS 인증서 갱신 | ACM 자동 발급/갱신 |
| ModSecurity WAF 규칙 유지 | AWS Managed Rules 구독 |
| istiod 컨트롤 플레인 모니터링 | 불필요 |
| Envoy sidecar 버전 관리 | 불필요 |

### 7.2 Spring Cloud Gateway = 개발팀 기술 스택 일치

URR 백엔드 팀이 Java/Spring 기반이므로, 라우팅과 필터를 Java 코드로 직접 제어할 수 있다.

- Rate Limiting: Redis Lua 스크립트 (`gateway-service/src/main/resources/redis/rate_limit.lua`)
- JWT 검증: Java Security Filter (`filter/JwtAuthFilter.java`)
- VWR 토큰 검증: Java Filter (`filter/VwrEntryTokenFilter.java`)
- Circuit Breaker: Resilience4j 애노테이션 (`@CircuitBreaker(name = "internalService")`)

Istio를 사용하면 이러한 로직이 Java 코드와 YAML CRD에 분산되어 디버깅이 복잡해진다.

### 7.3 규모 대비 Istio는 오버엔지니어링

| 판단 기준 | URR 현황 | Istio 권장 시점 |
|-----------|---------|-----------------|
| 마이크로서비스 수 | 8개 | 30개 이상 |
| 클러스터 수 | 1개 | 멀티 클러스터 |
| 서비스 간 통신 복잡도 | Gateway 중심 단방향 | 다대다 메시 통신 |
| 보안 규제 | 일반 | mTLS 의무화 환경 |
| 운영 인력 | 소규모 | 전담 플랫폼 팀 |

### 7.4 이미 대안으로 충분한 보안

- **서비스 간 인증**: `INTERNAL_API_TOKEN` + timing-safe 비교 (`InternalTokenValidator.java:29`)
- **네트워크 격리**: K8s NetworkPolicy로 서비스 간 접근 제어 (`k8s/spring/overlays/prod/network-policies.yaml`)
- **비동기 결합도 감소**: Kafka 이벤트 기반 통신으로 서비스 간 직접 호출 최소화
- **엣지 보안**: CloudFront + WAF + Lambda@Edge에서 대부분의 공격 차단

  - K8s 클러스터 내부 네트워크는 외부에서 직접 접근 불가
  - NetworkPolicy로 서비스 간 접근을 제한하고 있음
  - 엣지(CloudFront + WAF)에서 외부 공격은 이미 차단
  - 클러스터 내부 네트워크를 도청하려면 이미 클러스터가 뚫린 상태 → 그 시점에선 mTLS도 큰 의미 없음


### 7.5 티켓팅 시스템의 latency 요구

티켓 오픈 순간 수만 건의 동시 요청이 발생하는 환경에서, Envoy sidecar를 거치는 추가 ~1-3ms가 누적되면 사용자 체감에 영향을 줄 수 있다. 현재 구조는 프록시 레이어를 최소화하여 최단 경로로 요청을 처리한다.

```
URR 현재:    Client → CloudFront → ALB → Gateway Pod → Service Pod
Istio 사용 시: Client → CloudFront → ALB → Istio GW → Envoy → Gateway Pod → Envoy → Service Pod
                                                        (+1-3ms)              (+1-3ms)
```

---

## 8. 향후 전환 고려 시점

| 상황 | 권장 |
|------|------|
| 서비스 30개 이상으로 성장 | Istio 도입 검토 |
| 멀티 클러스터 / 멀티 리전 배포 | Istio 필수 |
| 규제 요구사항으로 mTLS 의무화 | Istio 도입 |
| AWS 종속 탈피 / 멀티 클라우드 | Nginx Ingress 전환 |
| 고급 카나리 (헤더 기반 트래픽 분배) | Istio VirtualService |
| 서비스 간 통신 관찰성 부족 체감 | Istio 또는 Linkerd (경량) 검토 |
| **현재 (8개 서비스, 단일 EKS)** | **현재 구조 유지 권장** |

### Istio 대신 경량 대안

Istio의 복잡도가 부담스러운 경우, 중간 단계로 고려할 수 있는 옵션:

| 대안 | 특징 | 적합 시점 |
|------|------|-----------|
| **Linkerd** | Istio 대비 경량 (Rust 프록시), 간단한 설정 | 서비스 15-30개 규모 |
| **AWS App Mesh** | AWS 네이티브 서비스 메시, Envoy 기반 | AWS 종속 유지 + mTLS 필요 시 |
| **Cilium** | eBPF 기반, sidecar 없음, L3/L4/L7 정책 | 성능 우선 + 네트워크 정책 강화 |

---

## 9. 결론

현재 URR의 **AWS ALB + Spring Cloud Gateway + Karpenter** 조합은 8개 서비스, 단일 EKS 클러스터 규모에서 **운영 부담 대비 가장 효율적인 선택**이다.

- Nginx Ingress를 추가하면 ALB와 역할이 중복되고, TLS/WAF를 직접 관리해야 한다.
- Istio를 추가하면 mTLS와 자동 관찰성을 얻지만, 리소스 오버헤드와 운영 복잡도가 크게 증가한다.
- Karpenter는 이미 적용되어 있어 티켓 오픈 시 빠른 노드 스케일링과 비용 최적화를 제공한다.

서비스 수가 30개 이상으로 증가하거나, 멀티 클러스터 배포가 필요하거나, mTLS가 규제 요구사항이 되는 시점에 Istio 또는 경량 대안을 재검토하면 된다.
