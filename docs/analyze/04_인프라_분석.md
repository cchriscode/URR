# URR 인프라 분석 문서

> 분석 대상: URR (티켓팅/예약 플랫폼) 인프라 전체
> 분석 일자: 2026-02-17
> 분석 범위: Terraform 모듈 20개, K8s 매니페스트, CI/CD 파이프라인, GitOps 구성

---

## 1. 인프라 개요

### 1-1. AWS 리전 및 환경 구성

| 항목 | 값 | 출처 |
|------|-----|------|
| AWS 리전 | `ap-northeast-2` (서울) | `terraform/environments/prod/variables.tf:8` |
| 보조 리전 | `us-east-1` (Lambda@Edge, WAF) | `terraform/environments/prod/main.tf:35` |
| IaC 도구 | Terraform >= 1.9 | `terraform/environments/prod/main.tf:2` |
| AWS Provider | ~> 5.0 | `terraform/environments/prod/main.tf:7` |

### 1-2. 환경별 모듈 구성

**Staging 환경 (15개 모듈)** - `terraform/environments/staging/main.tf`

| # | 모듈 | 소스 | 라인 |
|---|------|------|------|
| 1 | iam | `modules/iam` | 24-30 |
| 2 | vpc | `modules/vpc` | 36-42 |
| 3 | secrets | `modules/secrets` | 48-54 |
| 4 | vpc_endpoints | `modules/vpc-endpoints` | 60-69 |
| 5 | eks | `modules/eks` | 75-94 |
| 6 | rds | `modules/rds` | 100-121 |
| 7 | elasticache | `modules/elasticache` | 127-140 |
| 8 | msk | `modules/msk` | 146-163 |
| 9 | alb | `modules/alb` | 169-178 |
| 10 | s3 | `modules/s3` | 184-191 |
| 11 | sqs | `modules/sqs` | 197-205 |
| 12 | dynamodb_vwr | `modules/dynamodb-vwr` | 211-216 |
| 13 | api_gateway_vwr | `modules/api-gateway-vwr` | 222-230 |
| 14 | lambda_vwr + lambda_worker | `modules/lambda-vwr`, `modules/lambda-worker` | 232-297 |
| 15 | monitoring | `modules/monitoring` | 303-309 |

**Prod 환경 (19개 모듈)** - `terraform/environments/prod/main.tf`

Staging의 15개 모듈에 추가로 다음 4개 모듈이 포함된다:

| # | 모듈 | 소스 | 라인 |
|---|------|------|------|
| 16 | cloudfront | `modules/cloudfront` | 243-270 |
| 17 | waf | `modules/waf` | 380-389 |
| 18 | route53 | `modules/route53` | 395-404 |
| 19 | ecr | `modules/ecr` | 410-415 |

### 1-3. Terraform State 관리

S3 백엔드 + DynamoDB 잠금을 사용한다.

**Prod** (`terraform/environments/prod/main.tf:11-17`):
```hcl
backend "s3" {
  bucket         = "urr-terraform-state-prod"
  key            = "prod/terraform.tfstate"
  region         = "ap-northeast-2"
  dynamodb_table = "urr-terraform-locks"
  encrypt        = true
}
```

**Staging** (`terraform/environments/staging/main.tf:11-17`):
```hcl
backend "s3" {
  bucket         = "urr-terraform-state-staging"
  key            = "staging/terraform.tfstate"
  region         = "ap-northeast-2"
  dynamodb_table = "urr-terraform-locks"
  encrypt        = true
}
```

State 버킷 생성 절차는 `terraform/shared/backend.tf:1-28`에 주석으로 문서화되어 있다. S3 버전 관리 활성화, AES256 암호화, DynamoDB `PAY_PER_REQUEST` 과금 모드를 사용한다.

### 1-4. 기본 태그 전략

모든 리소스에 다음 태그가 자동 부여된다 (`terraform/environments/prod/main.tf:23-29`):
```hcl
default_tags {
  tags = {
    Project     = "URR"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}
```

---

## 2. AWS 서비스 아키텍처

### 2-1. 네트워크 (VPC)

**모듈**: `terraform/modules/vpc/main.tf`

#### VPC 기본 구성

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| CIDR | `10.0.0.0/16` | `10.1.0.0/16` | `prod/variables.tf:30`, `staging/variables.tf:30` |
| DNS Hostnames | true | true | `modules/vpc/main.tf:22` |
| DNS Support | true | true | `modules/vpc/main.tf:23` |
| AZ 수 | 2 (ap-northeast-2a, 2c) | 2 | `modules/vpc/main.tf:2` |

#### 서브넷 설계 (5-티어)

`modules/vpc/main.tf:1-9`에서 `cidrsubnet` 함수로 자동 계산된다:

| 서브넷 | CIDR 오프셋 | Prod CIDR 예시 | 용도 | 라인 |
|--------|------------|---------------|------|------|
| **Public** | k+0 | `10.0.0.0/24`, `10.0.1.0/24` | ALB, NAT GW, EKS ENI | `main.tf:5` |
| **App** | k+10 | `10.0.10.0/24`, `10.0.11.0/24` | EKS 노드, RDS Proxy | `main.tf:6` |
| **DB** | k+20 | `10.0.20.0/24`, `10.0.21.0/24` | RDS PostgreSQL | `main.tf:7` |
| **Cache** | k+30 | `10.0.30.0/24`, `10.0.31.0/24` | ElastiCache Redis | `main.tf:8` |
| **Streaming** | k+40 | `10.0.40.0/24`, `10.0.41.0/24` | MSK Kafka, Lambda Worker | `main.tf:9` |

#### NAT Gateway

AZ별 1개씩 총 2개의 NAT Gateway를 생성하여 고가용성을 확보한다 (`modules/vpc/main.tf:96-105`). 각 NAT GW는 퍼블릭 서브넷에 배치되며, App 서브넷과 Streaming 서브넷의 라우팅 테이블이 해당 AZ의 NAT GW를 참조한다 (`main.tf:135-141`, `230-236`).

DB 서브넷과 Cache 서브넷은 NAT GW 라우트가 없는 완전 격리 서브넷이다 (`main.tf:166-179`). DB 라우트 테이블을 Cache 서브넷과 공유한다 (`main.tf:197-202`).

#### Internet Gateway

`modules/vpc/main.tf:34-40`에서 IGW를 생성하고, 퍼블릭 라우트 테이블의 `0.0.0.0/0`이 IGW를 가리킨다 (`main.tf:68-72`).

#### VPC Endpoints

**모듈**: `terraform/modules/vpc-endpoints/main.tf`

**Interface Endpoints (PrivateLink)** - 9개:

| 엔드포인트 | 용도 | 라인 |
|-----------|------|------|
| `ec2` | EKS 노드 관리 | `main.tf:46-57` |
| `ecr.api` | ECR 이미지 풀링 (API) | `main.tf:60-71` |
| `ecr.dkr` | ECR 이미지 풀링 (Docker) | `main.tf:74-85` |
| `eks` | EKS API 호출 | `main.tf:88-99` |
| `sts` | IRSA 역할 위임 | `main.tf:102-113` |
| `logs` | CloudWatch Logs 전송 | `main.tf:116-127` |
| `secretsmanager` | RDS Proxy, 앱 시크릿 | `main.tf:130-141` |
| `elasticloadbalancing` | ALB 관리 | `main.tf:144-155` |
| `autoscaling` | 노드 그룹 오토스케일링 | `main.tf:158-169` |

모든 Interface Endpoint는 App 서브넷에 배치되며, VPC CIDR에서의 HTTPS(443) 인바운드만 허용하는 보안 그룹을 공유한다 (`main.tf:5-38`).

**Gateway Endpoints** - 2개:

| 엔드포인트 | 용도 | 라인 |
|-----------|------|------|
| `s3` | ECR 이미지 레이어, 프론트엔드 에셋 | `main.tf:176-185` |
| `dynamodb` | VWR 테이블 (조건부, 기본값 disabled) | `main.tf:188-198` |

Gateway Endpoint는 모든 라우트 테이블에 연결된다 (`main.tf:180`).

---

### 2-2. 컴퓨팅 (EKS)

**모듈**: `terraform/modules/eks/main.tf`

#### 클러스터 설정

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| K8s 버전 | 1.31 | 1.31 | `prod/variables.tf:40`, `staging/variables.tf:40` |
| 퍼블릭 엔드포인트 | true | true | `prod/variables.tf:46` |
| 프라이빗 엔드포인트 | true | true | `modules/eks/main.tf:106` |
| 퍼블릭 CIDR | `0.0.0.0/0` | `0.0.0.0/0` | `prod/variables.tf:52` |
| KMS 암호화 | null (AWS 관리형) | null | `prod/variables.tf:58` |

클러스터 로그 타입 5가지를 모두 활성화한다 (`modules/eks/main.tf:111`):
```
api, audit, authenticator, controllerManager, scheduler
```

CloudWatch 로그 보존 기간은 기본 7일이다 (`modules/eks/variables.tf:128`).

#### 노드 그룹

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| 인스턴스 타입 | `t3.medium` | `t3.small` | `prod/variables.tf:82`, `staging/variables.tf:82` |
| Desired | 3 | 2 | `prod/variables.tf:64`, `staging/variables.tf:64` |
| Min | 2 | 1 | `prod/variables.tf:70`, `staging/variables.tf:70` |
| Max | 5 | 3 | `prod/variables.tf:76`, `staging/variables.tf:76` |
| Capacity Type | ON_DEMAND | SPOT | `prod/variables.tf:88`, `staging/variables.tf:88` |
| 디스크 크기 | 20GB | 20GB | `modules/eks/variables.tf:94` |

노드 그룹의 desired_size는 `ignore_changes`로 관리된다 (`modules/eks/main.tf:187`). 즉 Terraform 외부(HPA, Karpenter)에서 변경한 값을 덮어쓰지 않는다.

#### OIDC Provider (IRSA)

`modules/eks/main.tf:133-145`에서 OIDC Provider를 생성한다. 이를 통해 K8s Service Account에 IAM 역할을 바인딩하는 IRSA 패턴을 사용할 수 있다.

#### Karpenter Autoscaler

`modules/eks/main.tf:351-439`에서 Karpenter Controller용 IRSA 역할을 생성한다. 이 역할에는 다음 권한이 부여된다:
- EC2 Fleet 생성/삭제, 인스턴스 관리 (`main.tf:395-411`)
- `iam:PassRole`로 노드 역할 전달 (`main.tf:416-419`)
- `karpenter.sh/nodepool` 태그 조건부 인스턴스 종료 (`main.tf:421-430`)
- EKS 클러스터 정보 조회 (`main.tf:432-436`)

NodePool 정의는 `k8s/karpenter/nodepool.yaml:8-41`에 있으며, 허용 인스턴스 타입은 `t3.medium`, `t3.large`, `t3.xlarge`, `m5.large`, `m5.xlarge`이다 (라인 26-31). CPU 제한 32코어, 메모리 제한 64Gi를 설정한다 (라인 37-38). `WhenEmptyOrUnderutilized` 통합 정책으로 60초 후 미사용 노드를 정리한다 (라인 40-41).

#### EKS Add-ons (4개)

| 애드온 | 용도 | IRSA | 라인 |
|--------|------|------|------|
| `vpc-cni` | Pod 네트워킹 | vpc-cni IRSA 역할 | `modules/eks/main.tf:196-211` |
| `kube-proxy` | 서비스 프록시 | 없음 | `main.tf:214-224` |
| `coredns` | DNS 해석 | 없음 | `main.tf:227-241` |
| `aws-ebs-csi-driver` | PV 관리 | ebs-csi IRSA 역할 | `main.tf:244-259` |

VPC CNI IRSA 역할은 `AmazonEKS_CNI_Policy`를 사용한다 (`main.tf:299-302`). EBS CSI IRSA 역할은 `AmazonEBSCSIDriverPolicy`를 사용한다 (`main.tf:342-344`).

---

### 2-3. 데이터베이스 (RDS)

**모듈**: `terraform/modules/rds/main.tf`

#### PostgreSQL 인스턴스

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| 엔진 버전 | PostgreSQL 16.4 | 16.4 | `prod/variables.tf:98` |
| 인스턴스 클래스 | `db.t3.medium` | `db.t3.medium` | `prod/variables.tf:104` |
| 할당 스토리지 | 50GB | 20GB | `prod/variables.tf:110`, `staging/variables.tf:110` |
| 최대 스토리지 (자동확장) | 100GB | 100GB | `modules/rds/variables.tf:55` |
| 스토리지 타입 | gp3 | gp3 | `modules/rds/main.tf:58` |
| 스토리지 암호화 | true | true | `modules/rds/main.tf:59` |
| Multi-AZ | **true** | **false** | `prod/main.tf:143`, `staging/main.tf:117` |
| RDS Proxy | true | true | `prod/main.tf:144`, `staging/main.tf:118` |
| Read Replica | **true** | **false** | `prod/main.tf:146`, `staging/main.tf:120` |
| 삭제 보호 | **true** | **false** | `prod/main.tf:145`, `staging/main.tf:119` |

#### 데이터베이스 구성

초기 DB는 `ticket_db`이며 (`modules/rds/variables.tf:61`), 추가 DB인 `auth_db`, `payment_db`, `stats_db`, `community_db`는 Flyway 마이그레이션이나 init 스크립트로 생성해야 한다 (`modules/rds/variables.tf:65-68`, `modules/rds/main.tf:62-66` 주석).

마스터 사용자는 `urr_admin`이다 (`terraform/environments/prod/main.tf:78`).

#### 파라미터 그룹

`modules/rds/main.tf:113-142`, family `postgres16`:

| 파라미터 | 값 | 목적 |
|---------|-----|------|
| `shared_preload_libraries` | `pg_stat_statements` | 쿼리 통계 수집 |
| `log_statement` | `ddl` | DDL 문 로깅 |
| `log_min_duration_statement` | `1000` | 1초 이상 슬로우 쿼리 로깅 |

#### 백업 및 모니터링

- 백업 보존: 7일 (`modules/rds/variables.tf:101`)
- 백업 윈도우: `03:00-04:00 UTC` (`modules/rds/main.tf:83`)
- 유지보수 윈도우: `mon:04:00-mon:05:00 UTC` (`main.tf:84`)
- Performance Insights: 활성화, 7일 보존 (`main.tf:91-92`)
- Enhanced Monitoring: 60초 간격 (`modules/rds/variables.tf:122`)
- CloudWatch Logs: `postgresql`, `upgrade` 로그 내보내기 (`main.tf:90`)

#### RDS Proxy

`modules/rds/main.tf:148-190`에서 커넥션 풀링을 제공한다.

- 엔진: POSTGRESQL (`main.tf:152`)
- 인증: Secrets Manager 기반 (`main.tf:153-157`)
- TLS 필수: true (`main.tf:162`)
- 유휴 타임아웃: 1800초 (30분) (`main.tf:163`)
- 커넥션 풀: 최대 100%, 유휴 50% (`main.tf:177-180`)
- 배치: APP 서브넷 (DB 서브넷이 아님) (`main.tf:160`, 주석 참조)

보안 그룹 규칙:
- EKS 노드에서 5432 인바운드 허용 (`main.tf:210-220`)
- Lambda Worker에서 5432 인바운드 허용 (`main.tf:223-233`)
- RDS로 5432 아웃바운드 허용 (`main.tf:236-246`)

#### Read Replica (Prod 전용)

`modules/rds/main.tf:265-287`에서 읽기 전용 복제본을 생성한다. Primary와 동일 인스턴스 클래스를 사용하고, Performance Insights와 Enhanced Monitoring이 동일하게 적용된다.

---

### 2-4. 캐시 (ElastiCache)

**모듈**: `terraform/modules/elasticache/main.tf`

#### Redis 클러스터 구성

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| 엔진 버전 | 7.1 | 7.1 | `modules/elasticache/variables.tf:32` |
| 노드 타입 | `cache.r6g.large` | `cache.t4g.small` | `prod/variables.tf:137`, `staging/variables.tf:137` |
| 클러스터 수 | 2 (Primary+Replica) | 1 (Primary만) | `prod/main.tf:165`, `staging/main.tf:139` |
| 자동 장애 조치 | true (클러스터 > 1) | false | `modules/elasticache/main.tf:100` |
| Multi-AZ | true (클러스터 > 1) | false | `main.tf:101` |
| AUTH 토큰 | 활성화 | 활성화 | `prod/main.tf:163`, `staging/main.tf:137` |

#### 보안 설정

- 저장 시 암호화: true (`modules/elasticache/main.tf:112`)
- 전송 중 암호화: true (`main.tf:113`)
- AUTH 토큰: Secrets Manager에서 생성 (`main.tf:114`)

#### 파라미터 그룹

`modules/elasticache/main.tf:59-82`, family `redis7`:

| 파라미터 | 값 | 목적 |
|---------|-----|------|
| `timeout` | 300 | 5분 유휴 타임아웃 |
| `maxmemory-policy` | `allkeys-lru` | 메모리 가득 시 LRU 키 퇴거 |

#### 백업 및 로깅

- 스냅샷 보존: 5일 (`modules/elasticache/variables.tf:81`)
- 스냅샷 윈도우: `03:00-05:00 UTC` (`main.tf:118`)
- 유지보수 윈도우: `sun:05:00-sun:07:00 UTC` (`main.tf:117`)
- Slow-log: CloudWatch JSON 포맷 (`main.tf:123-128`)
- Engine-log: CloudWatch JSON 포맷 (`main.tf:130-135`)

---

### 2-5. 메시징 (MSK)

**모듈**: `terraform/modules/msk/main.tf`

#### Kafka 클러스터 구성

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| Kafka 버전 | 3.6.0 | 3.6.0 | `prod/main.tf:180`, `staging/main.tf:154` |
| 브로커 수 | 2 | 2 | `prod/main.tf:181` |
| 인스턴스 타입 | `kafka.t3.small` | `kafka.t3.small` | `prod/variables.tf:121` |
| EBS 볼륨 | 50GB | 20GB | `prod/variables.tf:127`, `staging/variables.tf:127` |
| 기본 파티션 수 | 3 | 3 | `prod/main.tf:184` |
| TLS | 활성화 | 활성화 | `prod/main.tf:192` |
| IAM 인증 | 활성화 | 활성화 | `prod/main.tf:193` |
| Plaintext | 비활성화 | 비활성화 | `prod/main.tf:194` |

#### 서버 프로퍼티

`modules/msk/main.tf:98-113`:

| 프로퍼티 | 값 | 설명 |
|---------|-----|------|
| `auto.create.topics.enable` | true | 토픽 자동 생성 |
| `default.replication.factor` | 2 (브로커 > 1) | 기본 복제 계수 |
| `min.insync.replicas` | 1 (브로커 <= 2) | 최소 동기 복제본 |
| `num.partitions` | 3 | 기본 파티션 수 |
| `unclean.leader.election.enable` | false | 데이터 무결성 보장 |
| `log.retention.hours` | 168 (7일) | 로그 보존 기간 |

`min.insync.replicas` 로직 (`main.tf:101`):
```
broker_nodes > 2 ? 2 : 1
```
현재 브로커 2대이므로 `min.insync.replicas=1`이다. 이는 브로커 1대 장애 시에도 쓰기가 가능하지만, 데이터 유실 가능성이 있다.

#### 토픽 (Spring Kafka 자동 생성)

`prod/main.tf:186-190` 주석에 명시:
- `payment-events`
- `reservation-events`
- `transfer-events`
- `membership-events`

#### 암호화 및 모니터링

- 전송 중 암호화: TLS (`modules/msk/main.tf:163`)
- 클러스터 내 암호화: true (`main.tf:164`)
- 저장 시 암호화: AWS 관리형 KMS (`main.tf:167`)
- Enhanced Monitoring: `PER_TOPIC_PER_BROKER` (`modules/msk/variables.tf:104`)
- CloudWatch 로그: 활성화, 7일 보존 (`main.tf:120-127`, `variables.tf:110`)

#### CloudWatch 알람

- `ActiveControllerCount < 1`: 활성 컨트롤러 부재 알람 (`main.tf:202-220`)
- `OfflinePartitionsCount > 0`: 오프라인 파티션 알람 (`main.tf:222-240`)

---

### 2-6. 큐 (SQS)

**모듈**: `terraform/modules/sqs/main.tf`

#### FIFO 큐

`modules/sqs/main.tf:5-29`:

| 항목 | 값 | 출처 |
|------|-----|------|
| 큐 이름 | `{prefix}-ticket-events.fifo` | `main.tf:6` |
| FIFO | true | `main.tf:7` |
| Content-based dedup | true | `main.tf:8` |
| Dedup 범위 | `messageGroup` | `main.tf:9` |
| 처리량 제한 | `perMessageGroupId` | `main.tf:10` |
| 메시지 보존 | 345,600초 (4일) | `variables.tf:9` |
| 가시성 타임아웃 | 300초 (5분) | `variables.tf:15` |
| Long Polling | 10초 | `main.tf:15` |
| 암호화 | SQS 관리형 SSE | `main.tf:24` |

#### Dead Letter Queue

`modules/sqs/main.tf:32-41`:
- 이름: `{prefix}-ticket-events-dlq.fifo`
- 메시지 보존: 14일 (`main.tf:35`)
- SSE 암호화: true (`main.tf:36`)
- Redrive 정책: 최대 3회 수신 후 DLQ로 이동 (`main.tf:18-21`, `variables.tf:20`)

#### 큐 정책

`modules/sqs/main.tf:47-94`:
- EKS 노드(ticket-service)에 `sqs:SendMessage` 허용 (`main.tf:49-65`)
- Lambda Worker에 `sqs:ReceiveMessage`, `sqs:DeleteMessage` 허용 (`main.tf:68-88`)

#### CloudWatch 알람

- DLQ 메시지 존재 시 알람 (`main.tf:100-118`)
- 메시지 체류 10분 초과 시 알람 (`main.tf:120-138`)

---

### 2-7. CDN (CloudFront) - Prod 전용

**모듈**: `terraform/modules/cloudfront/main.tf`

#### 오리진 구성 (3개)

| 오리진 ID | 대상 | 프로토콜 | 라인 |
|----------|------|---------|------|
| `alb` | ALB DNS | HTTPS only, TLSv1.2 | `main.tf:91-106` |
| `s3` | S3 프론트엔드 버킷 | OAC (SigV4) | `main.tf:109-113` |
| `vwr-api` | API Gateway (VWR) | HTTPS only, TLSv1.2 | `main.tf:116-130` |

ALB 오리진에 `X-Custom-Header` 커스텀 헤더를 추가하여 CloudFront 경유 트래픽만 허용한다 (`main.tf:102-105`).

#### 캐시 동작 (5개)

| 패턴 | 오리진 | 캐시 | Lambda/Function | 라인 |
|------|--------|------|-----------------|------|
| `/*` (기본) | ALB | SSR 정책 (TTL 0-60s) | 없음 | `main.tf:133-143` |
| `/api/*` | ALB | API 정책 (TTL 0) | Lambda@Edge (viewer-request) | `main.tf:146-163` |
| `/_next/static/*` | S3 | Managed-CachingOptimized | 없음 | `main.tf:166-179` |
| `/vwr/*` | S3 | VWR Static (TTL 300s) | CF Function (page-rewrite) | `main.tf:183-199` |
| `/vwr-api/*` | API GW | API 정책 (TTL 0) | CF Function (api-rewrite) | `main.tf:203-222` |

#### Lambda@Edge

`modules/cloudfront/main.tf:48-64`:
- 함수명: `{prefix}-edge-queue-check`
- 런타임: `nodejs20.x`
- 배포 리전: `us-east-1` (필수)
- 타임아웃: 5초, 메모리: 128MB
- 역할: `viewer-request` 이벤트에서 대기열 토큰 검증

Lambda@Edge는 환경변수를 사용할 수 없으므로, `config.json`에 시크릿을 빌드 시 주입한다 (`main.tf:23-29`).

#### CloudFront Functions (2개)

| 함수 | 용도 | 라인 |
|------|------|------|
| `vwr-api-rewrite` | `/vwr-api` 접두사 제거 후 API GW 전달 | `main.tf:422-437` |
| `vwr-page-rewrite` | `/vwr/{eventId}` -> `/vwr/index.html` 리라이트 | `main.tf:440-453` |

#### OAC (Origin Access Control)

`modules/cloudfront/main.tf:70-76`: S3 오리진에 대해 SigV4 서명 방식의 OAC를 사용한다.

#### 캐시 정책

- **Frontend SSR** (`main.tf:252-278`): TTL 0-60초, 쿠키 전체 전달, Authorization/Host 헤더 포함
- **API** (`main.tf:305-331`): TTL 0, 캐싱 없음, 전체 쿠키/쿼리스트링 전달
- **VWR Static** (`main.tf:280-303`): TTL 300초, 쿠키/헤더/쿼리스트링 없음

#### 보안 응답 헤더

`modules/cloudfront/main.tf:361-412`:
- HSTS: `max-age=31536000; includeSubDomains; preload`
- X-Content-Type-Options: `nosniff`
- X-Frame-Options: `DENY`
- X-XSS-Protection: `1; mode=block`
- Referrer-Policy: `strict-origin-when-cross-origin`
- CORS: `cors_allowed_origins` 변수 기반

#### 기타 설정

- Price Class: `PriceClass_200` (`prod/main.tf:259`)
- IPv6: 활성화 (`main.tf:84`)
- 최소 TLS: `TLSv1.2_2021` (`main.tf:236`)
- WAF 연동: `web_acl_arn` 바인딩 (`main.tf:88`, `prod/main.tf:264`)

---

### 2-8. DNS (Route53) - Prod 전용

**모듈**: `terraform/modules/route53/main.tf`

#### Hosted Zone

`modules/route53/main.tf:5-14`: `create_hosted_zone` 변수에 따라 신규 생성 또는 기존 Zone ID를 사용한다.

#### DNS 레코드

| 레코드 | 타입 | 대상 | 라인 |
|--------|------|------|------|
| `{domain_name}` | A (Alias) | CloudFront 배포 | `main.tf:21-32` |
| `www.{domain_name}` | A (Alias) | CloudFront 배포 (선택) | `main.tf:35-46` |

Prod 호출부 (`prod/main.tf:395-404`):
```hcl
domain_name               = var.domain_name
cloudfront_domain_name    = module.cloudfront.distribution_domain_name
cloudfront_hosted_zone_id = module.cloudfront.distribution_hosted_zone_id
```

---

### 2-9. 보안 (WAF, IAM)

#### WAF - Prod 전용

**모듈**: `terraform/modules/waf/main.tf`

CLOUDFRONT 스코프로 `us-east-1`에 배포된다 (`main.tf:14`, `18`).

**규칙 (4개)**:

| 우선순위 | 규칙 | 유형 | 동작 | 라인 |
|---------|------|------|------|------|
| 1 | Rate Limiting | IP 기반 속도 제한 | Block | `main.tf:25-45` |
| 2 | AWSManagedRulesCommonRuleSet | 일반 공격 방어 | None (managed) | `main.tf:48-68` |
| 3 | AWSManagedRulesKnownBadInputsRuleSet | 알려진 악성 입력 차단 | None (managed) | `main.tf:71-91` |
| 4 | AWSManagedRulesSQLiRuleSet | SQL 인젝션 방어 | None (managed) | `main.tf:94-114` |

Rate Limit: IP당 5분간 최대 2,000 요청 (`prod/variables.tf:199`).

#### IAM 역할

**모듈**: `terraform/modules/iam/main.tf`

| 역할 | Trust | 주요 정책 | 라인 |
|------|-------|---------|------|
| EKS Cluster Role | `eks.amazonaws.com` | `AmazonEKSClusterPolicy`, `AmazonEKSVPCResourceController` | `main.tf:18-31` |
| EKS Node Role | `ec2.amazonaws.com` | `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, `AmazonEC2ContainerRegistryReadOnly`, `AmazonSSMManagedInstanceCore` | `main.tf:50-73` |
| Lambda Worker Role | `lambda.amazonaws.com` | `AWSLambdaBasicExecutionRole`, `AWSLambdaVPCAccessExecutionRole`, SQS ReceiveMessage/DeleteMessage | `main.tf:92-126` |
| Lambda@Edge Role | `lambda.amazonaws.com`, `edgelambda.amazonaws.com` | `AWSLambdaBasicExecutionRole` | `main.tf:148-156` |
| RDS Monitoring Role | `monitoring.rds.amazonaws.com` | `AmazonRDSEnhancedMonitoringRole` | `main.tf:175-183` |
| RDS Proxy Role | `rds.amazonaws.com` | Secrets Manager `GetSecretValue` | `main.tf:202-224` |

추가로 EKS 모듈 내에서 3개의 IRSA 역할을 생성한다:
- VPC CNI IRSA (`modules/eks/main.tf:290-302`)
- EBS CSI IRSA (`modules/eks/main.tf:333-344`)
- Karpenter Controller IRSA (`modules/eks/main.tf:376-439`)

---

### 2-10. 서버리스 (Lambda, API Gateway, DynamoDB)

#### VWR Lambda 함수

**모듈**: `terraform/modules/lambda-vwr/main.tf`

| 함수 | 런타임 | 목적 | 라인 |
|------|--------|------|------|
| `{prefix}-vwr-api` | nodejs20.x | VWR 대기열 위치 할당/조회 | `main.tf:13-37` |
| `{prefix}-vwr-counter-advancer` | nodejs20.x | 서빙 카운터 10초 주기 증가 | `main.tf:107-127` |

VWR API Lambda:

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| 예약 동시 실행 | 500 | 20 | `prod/main.tf:303`, `staging/main.tf:238` |
| Counter Batch Size | 500 | 100 | `prod/main.tf:304`, `staging/main.tf:239` |

Counter Advancer는 EventBridge 규칙으로 1분마다 트리거되며, Lambda 내부에서 10초 간격 루프를 실행한다 (`main.tf:130-136`, 주석 참조). 타임아웃은 70초로 6회 x 10초 + 마진이다 (`main.tf:112`).

#### Lambda Worker (SQS Consumer)

**모듈**: `terraform/modules/lambda-worker/main.tf`

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| 타임아웃 | 30초 | 30초 | `prod/main.tf:329` |
| 메모리 | 256MB | 256MB | `prod/main.tf:330` |
| 예약 동시 실행 | 10 | 5 | `prod/main.tf:331`, `staging/main.tf:266` |
| SQS 배치 크기 | 10 | 10 | `prod/main.tf:353` |
| 배치 윈도우 | 5초 | 5초 | `prod/main.tf:354` |
| 최대 동시성 | 10 | 5 | `prod/main.tf:355`, `staging/main.tf:291` |
| X-Ray | Active | Active | `prod/main.tf:359` |

VPC 내에 배치되어 RDS Proxy, Redis, Kafka에 접근한다 (`modules/lambda-worker/main.tf:86-89`). 환경변수로 `DB_PROXY_ENDPOINT`, `REDIS_ENDPOINT`, `KAFKA_BOOTSTRAP_SERVERS`, `TICKET_SERVICE_URL` 등을 주입한다 (`main.tf:92-102`).

SQS 이벤트 소스 매핑에서 `ReportBatchItemFailures`를 사용하여 부분 실패를 처리한다 (`main.tf:129`).

CloudWatch 알람 3개: Errors > 5, Duration > 80% timeout, Throttles > 0 (`main.tf:153-211`).

#### API Gateway

**모듈**: `terraform/modules/api-gateway-vwr/main.tf`

REST API (REGIONAL) 엔드포인트:

| 리소스 | 메서드 | 용도 | 라인 |
|--------|--------|------|------|
| `/vwr/assign/{eventId}` | POST | 대기열 위치 할당 | `main.tf:36-51` |
| `/vwr/check/{eventId}/{requestId}` | GET | 위치 확인 | `main.tf:75-95` |
| `/vwr/status/{eventId}` | GET | 이벤트 상태 조회 | `main.tf:113-128` |

모든 엔드포인트에 OPTIONS CORS 지원 (`main.tf:134-177`).

스로틀링 설정:

| 항목 | Prod | Staging | 출처 |
|------|------|---------|------|
| Burst Limit | 10,000 | 2,000 | `prod/main.tf:293`, `staging/main.tf:228` |
| Rate Limit | 5,000 | 1,000 | `prod/main.tf:294`, `staging/main.tf:229` |

#### DynamoDB

**모듈**: `terraform/modules/dynamodb-vwr/main.tf`

| 테이블 | 파티션 키 | 정렬 키 | 과금 | PITR | 라인 |
|--------|----------|---------|------|------|------|
| `{prefix}-vwr-counters` | `eventId` (S) | - | PAY_PER_REQUEST | Prod: true, Staging: false | `main.tf:6-23` |
| `{prefix}-vwr-positions` | `eventId` (S) | `requestId` (S) | PAY_PER_REQUEST | Prod: true, Staging: false | `main.tf:26-68` |

Positions 테이블 특수 설정:
- TTL: `ttl` 속성으로 24시간 후 자동 삭제 (`main.tf:48-51`)
- GSI: `eventId-position-index` (해시: `eventId`, 범위: `position`, ALL 프로젝션) (`main.tf:54-59`)

PITR 설정: Prod true (`prod/main.tf:280`), Staging false (`staging/main.tf:215`).

---

### 2-11. 스토리지 (S3, ECR)

#### S3

**모듈**: `terraform/modules/s3/main.tf`

**Frontend 버킷** (`main.tf:5-12`):
- 이름: `{prefix}-frontend-{environment}`
- 버전 관리: 활성화 (`main.tf:19-25`)
- 서버 측 암호화: AES256 + Bucket Key (`main.tf:28-37`)
- 퍼블릭 액세스: 완전 차단 (`main.tf:40-47`)
- CORS: `cors_allowed_origins`, GET/HEAD 허용 (`main.tf:76-86`)

라이프사이클 규칙 (`main.tf:50-73`):
- 30일 후 비현재 버전 만료
- 7일 후 미완료 멀티파트 업로드 중단

CloudFront OAC 기반 버킷 정책 (`main.tf:92-123`): `s3:GetObject`만 허용, `AWS:SourceArn` 조건으로 특정 CloudFront 배포만 접근 가능.

**Logs 버킷** (조건부, `main.tf:129-262`):
- 이름: `{prefix}-logs-{environment}`
- 라이프사이클: 30일 후 STANDARD_IA, 90일 후 GLACIER, 만료일 설정 가능
- ALB 로그 쓰기 허용 정책 포함

#### ECR - Prod 전용

**모듈**: `terraform/modules/ecr/main.tf`

9개 레포지토리 생성 (`modules/ecr/variables.tf:9-19`):
```
gateway-service, auth-service, ticket-service, payment-service,
queue-service, stats-service, catalog-service, community-service, frontend
```

설정:
- 이미지 태그 불변성: `IMMUTABLE` (`main.tf:13`)
- Push 시 스캔: true (`main.tf:15-17`)
- 암호화: AES256 (`main.tf:19-21`)
- 최대 보관 이미지: 30개 (`prod/main.tf:414`)

라이프사이클 정책 (`main.tf:30-64`):
1. 태그 없는 이미지: 7일 후 만료
2. `v`, `prod`, `staging` 접두사 태그 이미지: 최대 30개 보관 후 초과분 만료

---

### 2-12. 시크릿 (Secrets Manager)

**모듈**: `terraform/modules/secrets/main.tf`

4개의 시크릿을 자동 생성한다:

| 시크릿 | 내용 | 비밀번호 길이 | 라인 |
|--------|------|-------------|------|
| `{prefix}/rds-credentials` | username, password, engine, host, port, dbname | 32자 (특수문자 포함) | `main.tf:5-28` |
| `{prefix}/redis-auth-token` | token | 32자 (특수문자 없음) | `main.tf:34-52` |
| `{prefix}/queue-entry-token-secret` | HMAC-SHA256 시크릿 | 64자 (특수문자 없음) | `main.tf:58-76` |
| `{prefix}/jwt-secret` | JWT 서명 시크릿 | 64자 (특수문자 없음) | `main.tf:82-100` |

모든 시크릿은 `random_password` 리소스로 자동 생성되며, RDS 자격증명에는 호스트/포트 정보가 포함된다 (`main.tf:20-27`).

---

### 2-13. 모니터링 (AMP, Grafana)

**모듈**: `terraform/modules/monitoring/main.tf`

#### Amazon Managed Prometheus (AMP)

`modules/monitoring/main.tf:5-11`: 워크스페이스 alias `{prefix}-amp`.

#### Amazon Managed Grafana (AMG)

`modules/monitoring/main.tf:17-34`:
- 인증: AWS SSO (`main.tf:21`)
- 권한: SERVICE_MANAGED (`main.tf:22`)
- 데이터 소스: PROMETHEUS (`main.tf:24`)
- 플러그인 관리: 활성화 (`main.tf:26-29`)

Grafana 워크스페이스 역할에 AMP 쿼리 권한 부여 (`main.tf:61-82`):
```
aps:ListWorkspaces, aps:QueryMetrics, aps:GetLabels, ...
```

#### Prometheus IRSA (Remote Write)

`modules/monitoring/main.tf:88-140`: EKS의 `kube-prometheus-stack-prometheus` 서비스 어카운트에 AMP Remote Write 권한을 부여하는 IRSA 역할.

서비스 어카운트: `system:serviceaccount:monitoring:kube-prometheus-stack-prometheus` (`main.tf:101`)

---

## 3. K8s 배포 구조

### 3-1. Kustomize Base + Overlays 패턴

**디렉토리 구조**:
```
k8s/spring/
  base/                          # 공통 리소스 정의
    kustomization.yaml           # 리소스 목록
    {service}/deployment.yaml    # 9개 서비스 Deployment
    {service}/service.yaml       # 9개 서비스 Service
    network-policies.yaml        # 네트워크 정책
    catalog-service-pdb.yaml     # 카탈로그 PDB
  overlays/
    dev/kustomization.yaml       # 개발 환경
    staging/kustomization.yaml   # 스테이징 환경
    prod/kustomization.yaml      # 프로덕션 환경
```

### 3-2. 환경별 네임스페이스

| 환경 | 네임스페이스 | 출처 |
|------|------------|------|
| Dev | `urr-dev` | `k8s/spring/base/kustomization.yaml:3`, `overlays/dev/kustomization.yaml:3` |
| Staging | `urr-staging` | `overlays/staging/kustomization.yaml:3` |
| Prod | `urr-spring` | `overlays/prod/kustomization.yaml:10` |

### 3-3. ConfigMap/Secret 생성

Prod와 Staging에서 Kustomize `configMapGenerator`와 `secretGenerator`를 사용한다.

**Prod** (`overlays/prod/kustomization.yaml:27-37`):
```yaml
configMapGenerator:
  - name: spring-prod-config
    envs:
      - config.env
secretGenerator:
  - name: spring-prod-secret
    envs:
      - secrets.env
generatorOptions:
  disableNameSuffixHash: true
```

**Staging** (`overlays/staging/kustomization.yaml:8-19`):
```yaml
configMapGenerator:
  - name: spring-staging-config
    envs:
      - config.env
secretGenerator:
  - name: spring-staging-secret
    envs:
      - secrets.env
```

`disableNameSuffixHash: true`로 해시 접미사를 비활성화하여 이름 변경 없이 참조할 수 있다.

Deployment에서 `envFrom`으로 ConfigMap/Secret을 일괄 주입하고, 개별 `env`로 세밀한 설정을 오버라이드한다 (`overlays/prod/patches/services-env.yaml:10-14`).

### 3-4. 서비스 구성 (9개)

Base Deployment 공통 설정 (`k8s/spring/base/ticket-service/deployment.yaml` 기준):

| 항목 | 값 | 라인 |
|------|-----|------|
| securityContext.runAsNonRoot | true | `deployment.yaml:23` |
| securityContext.runAsUser | 1000 | `deployment.yaml:24` |
| allowPrivilegeEscalation | false | `deployment.yaml:31` |
| capabilities.drop | ALL | `deployment.yaml:33-34` |
| Prometheus scrape | true, `/actuator/prometheus` | `deployment.yaml:15-17` |
| startupProbe | `/actuator/health/liveness`, failureThreshold 30, period 5s | `deployment.yaml:43-48` |
| readinessProbe | `/actuator/health/readiness`, period 10s | `deployment.yaml:49-53` |
| livenessProbe | `/actuator/health/liveness`, period 20s | `deployment.yaml:54-58` |

리소스 요청/제한 (예: ticket-service):
- requests: cpu 200m, memory 256Mi (`deployment.yaml:60-62`)
- limits: cpu 1, memory 1Gi (`deployment.yaml:63-65`)

### 3-5. Prod 레플리카 수

`k8s/spring/overlays/prod/patches/replicas.yaml`:

| 서비스 | 레플리카 | 라인 |
|--------|---------|------|
| gateway-service | 3 | 6 |
| auth-service | 2 | 12 |
| ticket-service | 3 | 18 |
| payment-service | 2 | 24 |
| stats-service | 2 | 30 |
| queue-service | 3 | 36 |
| community-service | 2 | 42 |
| frontend | 2 | 48 |

### 3-6. HPA (Horizontal Pod Autoscaler) - Prod 전용

`k8s/spring/overlays/prod/hpa.yaml`:

| 서비스 | Min | Max | CPU 임계값 | 라인 |
|--------|-----|-----|-----------|------|
| gateway-service | 3 | 10 | 70% | 1-18 |
| ticket-service | 3 | 10 | 70% | 20-37 |
| queue-service | 3 | 8 | 70% | 39-56 |
| payment-service | 2 | 6 | 70% | 58-76 |

### 3-7. PDB (PodDisruptionBudget) - Prod 전용

`k8s/spring/overlays/prod/pdb.yaml`:

7개 서비스에 `minAvailable: 1` 적용 (라인 1-70):
gateway-service, ticket-service, queue-service, payment-service, auth-service, stats-service, community-service

### 3-8. NetworkPolicy

**Base** (`k8s/spring/base/network-policies.yaml`):

- `default-deny-all`: 모든 Ingress/Egress 차단 (라인 1-9)
- 서비스별 Ingress 허용:
  - gateway-service: 모든 소스에서 3001 포트 (라인 11-23)
  - frontend: 모든 소스에서 3000 포트 (라인 25-37)
  - 백엔드 서비스: gateway-service 및 특정 호출자에서만 허용 (라인 39-176)
- 서비스별 Egress 허용:
  - gateway-service -> `tier: backend` 라벨 Pod (라인 352-375)
  - ticket-service -> payment-service, `tier: data` (라인 198-224)
  - payment-service -> ticket-service, `tier: data` (라인 226-251)
  - 모든 서비스에 DNS(53) Egress 허용

### 3-9. Argo Rollouts (Blue-Green) - Prod 전용

4개 핵심 서비스에 Blue-Green 배포를 적용한다.

**Rollout 정의** (`k8s/spring/overlays/prod/rollouts/ticket-service.yaml` 기준):

```yaml
strategy:
  blueGreen:
    activeService: ticket-service
    previewService: ticket-service-preview
    autoPromotionEnabled: false
    prePromotionAnalysis:
      templates:
        - templateName: health-check
    scaleDownDelaySeconds: 30
```
(라인 43-57)

- `autoPromotionEnabled: false`: 수동 승격 필요
- `prePromotionAnalysis`: 승격 전 Health Check 수행
- `scaleDownDelaySeconds: 30`: 이전 버전 30초 후 스케일다운

**Preview Services** (`k8s/spring/overlays/prod/preview-services.yaml`):
4개 Preview Service 정의 (gateway:3001, ticket:3002, payment:3003, queue:3007)

**AnalysisTemplate** (`k8s/spring/overlays/prod/analysis-template.yaml`):
```yaml
metrics:
  - name: health-endpoint
    interval: 10s
    count: 5
    failureLimit: 1
    successCondition: result == 200
    provider:
      web:
        url: "http://{{args.service-name}}.urr-spring.svc.cluster.local:{{args.port}}/health"
```
(라인 11-18)

10초 간격으로 5회 Health Check, 1회 실패 허용.

---

## 4. CI/CD 파이프라인

### 4-1. GitHub Actions 워크플로우 구조

| 워크플로우 | 트리거 | 용도 | 파일 |
|-----------|--------|------|------|
| `reusable-spring-ci-cd.yml` | workflow_call | 재사용 가능한 빌드/배포 파이프라인 | `.github/workflows/reusable-spring-ci-cd.yml` |
| `{service}-ci-cd.yml` (8개) | push(main) + paths, workflow_dispatch | 서비스별 CI/CD | 예: `.github/workflows/ticket-service-ci-cd.yml` |
| `pr-validation.yml` | pull_request(main) | PR 변경 사항 검증 | `.github/workflows/pr-validation.yml` |
| `rollback.yml` | workflow_dispatch | 수동 롤백 | `.github/workflows/rollback.yml` |
| `e2e-tests.yml` | workflow_call | E2E 테스트 게이트 | `.github/workflows/e2e-tests.yml` |
| `load-tests.yml` | workflow_call | 부하 테스트 | `.github/workflows/load-tests.yml` |
| `frontend-ci-cd.yml` | push(main) + paths | 프론트엔드 빌드/배포 | `.github/workflows/frontend-ci-cd.yml` |

### 4-2. Reusable Workflow 패턴

`reusable-spring-ci-cd.yml`에서 입력 파라미터를 받아 모든 서비스에 동일한 파이프라인을 적용한다:

**입력** (라인 5-22):
- `service-name`: 서비스 이름 (필수)
- `ecr-repository`: ECR 레포 경로 (필수)
- `service-path`: 서비스 소스 경로 (필수)
- `environment`: 대상 환경 (기본: staging)

**시크릿** (라인 23-29):
- `AWS_ROLE_ARN`: OIDC 연동 IAM 역할
- `AWS_ACCOUNT_ID`: AWS 계정 ID
- `DISCORD_WEBHOOK`: 알림용

### 4-3. 파이프라인 단계

```
unit-test -> integration-test -> build-and-push -> update-manifests -> e2e-gate -> load-test -> notify
```

**1단계: Unit Tests** (라인 35-71)
- JDK 21 (Temurin)
- Gradle 캐시
- `./gradlew test`

**2단계: Integration Tests** (라인 73-111)
- Unit Test 성공 후 실행
- `./gradlew integrationTest` (없으면 건너뜀)

**3단계: Build & Push to ECR** (라인 112-203)
- OIDC로 AWS 인증 (`aws-actions/configure-aws-credentials@v4`, 라인 152-156)
- Docker Buildx로 `linux/arm64` 플랫폼 빌드 (라인 173)
- 이미지 태그: `{short-sha}-{timestamp}` (라인 146-150)
- ECR Push: 3개 태그 (`image-tag`, `latest`, `environment`) (라인 175-178)
- Trivy 보안 스캔: CRITICAL, HIGH 취약점 시 실패 (라인 186-193)
- GHA 캐시 활용 (라인 183-184)

**4단계: Update Kustomize Manifests** (라인 204-289)
- `kustomization.yaml`에서 해당 서비스의 `newName`과 `newTag`를 sed로 업데이트 (라인 245-248)
- 5회까지 rebase + push 재시도 (라인 262-275)
- 커밋 메시지: `[skip ci] chore(k8s): update {service} image to {tag} [{env}]`

**5단계: E2E Gate** (라인 290-296)
- Staging 환경일 때만 실행
- `e2e-tests.yml` 호출

**6단계: Load Test** (라인 298-305)
- Staging + E2E 성공 후 실행
- `browse-events` 시나리오

**7단계: Discord 알림** (라인 307-357)

### 4-4. Production Approval Gate

각 서비스 CI/CD 파일에서 `environment: production`을 사용하여 수동 승인을 요구한다.

`ticket-service-ci-cd.yml:21-28`:
```yaml
prod-approval:
  name: Production Approval
  if: github.event.inputs.environment == 'prod'
  runs-on: ubuntu-latest
  environment: production
  steps:
    - name: Approved
      run: echo "Production deployment approved for ticket-service"
```

### 4-5. PR Validation

`pr-validation.yml`:
- `dorny/paths-filter@v3`로 변경된 서비스 자동 감지 (라인 19-40)
- Matrix 전략으로 변경된 서비스만 병렬 테스트 (라인 42-65)
- 프론트엔드: npm ci, lint, build (라인 67-89)
- `fail-fast: false`로 한 서비스 실패가 다른 서비스에 영향 없음 (라인 50)

### 4-6. Rollback Workflow

`rollback.yml`:
- 서비스 선택 (9개 옵션) + 이미지 태그 입력 + 환경 선택 (라인 5-30)
- Prod는 `production` 환경 승인 필요 (라인 40)
- Kustomization 파일의 `newTag`를 지정 태그로 변경 (라인 80-103)
- 5회 재시도 push (라인 115-128)
- Discord 롤백 알림 (라인 133-164)

---

## 5. GitOps (ArgoCD)

### 5-1. ArgoCD Application 정의

3개 환경에 대한 Application이 정의되어 있다.

**Prod** (`argocd/applications/urr-spring-prod.yaml`):
```yaml
spec:
  source:
    repoURL: https://github.com/YOUR_ORG/YOUR_REPO.git
    targetRevision: main
    path: k8s/spring/overlays/prod
  destination:
    server: https://kubernetes.default.svc
    namespace: urr-spring
```
(라인 7-14)

**Staging** (`argocd/applications/urr-spring-staging.yaml:7-14`):
- path: `k8s/spring/overlays/staging`
- namespace: `urr-staging`

**Dev** (`argocd/applications/urr-spring-dev.yaml:7-14`):
- path: `k8s/spring/overlays/dev`
- namespace: `urr-dev`

### 5-2. Auto-Sync 설정

모든 환경에 동일한 syncPolicy를 적용한다 (`urr-spring-prod.yaml:15-20`):
```yaml
syncPolicy:
  automated:
    prune: true
    selfHeal: true
  syncOptions:
    - CreateNamespace=true
```

- `prune: true`: Git에서 제거된 리소스를 클러스터에서도 삭제
- `selfHeal: true`: 클러스터 리소스가 Git과 다를 경우 자동 복구
- `CreateNamespace=true`: 네임스페이스 자동 생성

### 5-3. 이미지 태그 업데이트 -> 배포 흐름

1. CI/CD 파이프라인이 새 이미지를 ECR에 Push
2. `update-manifests` Job이 `kustomization.yaml`의 `newTag`를 업데이트하고 Git에 Push (`reusable-spring-ci-cd.yml:229-278`)
3. ArgoCD가 Git 변경을 감지 (targetRevision: `main`)
4. ArgoCD가 자동으로 Sync 실행 (automated + selfHeal)
5. Prod에서 Argo Rollouts가 Blue-Green 배포 수행
6. prePromotionAnalysis Health Check 통과 후 수동 승격 대기

---

## 6. VWR 기반 트래픽 관리 및 오토스케일링 전략

### 6-1. VWR Two-Tier가 백엔드를 보호하는 구조

15,000명이 동시에 "예매하기"를 클릭해도 Spring 백엔드가 직접 15,000 요청을 처리하지 않는다. VWR이 트래픽을 제어한다.

```
15,000명 "예매하기" 클릭
    │
    ▼
━━ Tier 1: VWR (Lambda + DynamoDB) ━━━━━━━━━━━━━━━━━
    │  서버리스 → 자동 스케일링 → 15K 폴링 처리
    │  배치 입장 허가 (예: 500명씩)
    ▼
━━ Tier 2: queue-service (Redis) ━━━━━━━━━━━━━━━━━━━
    │  500명 대기열 관리 → 순서대로 50-100명씩 릴리스
    ▼
━━ 실제 Backend (ticket + payment) ━━━━━━━━━━━━━━━━━
    │  동시 50-200명만 좌석 선택 + 결제 처리
    ▼
    완료
```

### 6-2. 구간별 부하 수준

| 구간 | 15K 직접 받나? | 대응 방식 |
|------|---------------|-----------|
| VWR Tier 1 (Lambda) | 15K 폴링 | 서버리스 → **자동 스케일** |
| Gateway | 브라우징 + VWR 통과분 | **HPA 자동** |
| queue-service | 500-1000명 대기열 | **HPA 자동** + Redis |
| ticket/payment | 50-200명 | HPA + 기본 설정으로 충분 |
| stats/catalog/community | 낮은 부하 | 기본 설정 OK |

### 6-3. HPA + VWR 연동

VWR이 트래픽 급증을 흡수하는 동안 HPA가 Pod를 스케일업할 시간(30-60초)을 확보한다.

```
t=0s   15K 유저 접속 → VWR Tier 1이 즉시 흡수 (Lambda 자동 스케일)
t=10s  VWR이 첫 배치 500명 입장 허가 → queue-service CPU 상승
t=30s  HPA 감지 → queue-service Pod 3→5개 스케일업
t=60s  VWR이 지속적으로 배치 입장 → backend 안정적 처리
```

VWR 없이 15K가 직접 backend를 때리면 HPA 스케일업 전에 서비스가 다운될 수 있다. VWR이 이 30-60초 갭을 메워준다.

---

## 7. 오토스케일링 및 리소스 튜닝

### 7-1. HPA (Horizontal Pod Autoscaler) 전 서비스 적용

`k8s/spring/overlays/prod/hpa.yaml`에 8개 서비스 전부 HPA가 정의되어 있다.

| 서비스 | Min Replicas | Max Replicas | Scale Trigger | 비고 |
|--------|-------------|-------------|---------------|------|
| gateway-service | 3 | 10 | CPU 70% | 진입점, 높은 기본 Replica |
| ticket-service | 3 | 10 | CPU 70% | 핵심 예매 서비스 |
| queue-service | 3 | 8 | CPU 70% | VWR Tier 2, Redis 의존 |
| payment-service | 2 | 6 | CPU 70% | 결제 처리 |
| auth-service | 2 | 6 | CPU 70% | 로그인 급증 대비 |
| catalog-service | 2 | 6 | CPU 70% | 이벤트 목록 조회 |
| stats-service | 1 | 4 | CPU 70% | 비동기 Kafka 소비, 부하 낮음 |
| community-service | 1 | 4 | CPU 70% | 게시판, 부하 낮음 |

### 7-2. JVM 힙 메모리 설정

모든 서비스의 Dockerfile에 JVM 옵션이 명시적으로 설정되어 있다:

```dockerfile
ENTRYPOINT ["java", "-XX:MaxRAMPercentage=75.0", "-XX:InitialRAMPercentage=50.0", "-XX:+UseG1GC", "-jar", "/app/app.jar"]
```

| 설정 | 값 | 의미 |
|------|-----|------|
| `MaxRAMPercentage` | 75% | 컨테이너 메모리 한도의 75%를 힙으로 사용 |
| `InitialRAMPercentage` | 50% | 시작 시 50% 할당 (콜드스타트 GC 최소화) |
| `UseG1GC` | G1 GC | 마이크로서비스에 적합한 GC (1GB+ 힙에서 효율적) |

K8s 메모리 limit이 1Gi인 경우: 힙 최대 ~768MB, 시작 시 ~512MB 할당.

### 7-3. Gateway Redis 커넥션 풀

Rate Limiting이 모든 요청마다 Redis를 호출하므로, gateway-service에 Lettuce 커넥션 풀이 설정되어 있다:

```yaml
# gateway-service/application.yml
spring.data.redis.lettuce.pool:
  max-active: 32
  max-idle: 16
  min-idle: 4
```

의존성: `org.apache.commons:commons-pool2` (gateway-service/build.gradle)

### 7-4. 과잉 튜닝 방지 (VWR 보호 하)

VWR이 트래픽을 제어하므로 아래 설정은 **기본값으로 충분**하다:

| 설정 | 기본값 | 과잉 튜닝? | 이유 |
|------|--------|-----------|------|
| DB 커넥션 풀 (HikariCP) | 10 | VWR 하 50-200명이면 충분 | 필요 시 환경변수로 조정 가능 |
| Tomcat 스레드 | 200 | VWR 하 충분 | Gateway 3 Pod × 200 = 600 스레드 |
| Kafka Consumer 스레드 | 1/리스너 | VWR 하 충분 | 이벤트 수백건/분 수준 |

이 설정들은 VWR 없이 15K를 직접 받을 때만 튜닝이 필요하다.

---

## 8. 15K 버스트 트래픽 준비도 평가

15,000명이 동시에 "예매하기"를 클릭하는 시나리오를 기준으로, 유저 여정 전체 구간(브라우징 → VWR → 큐 → 예매 → 결제 → 사후)에 대해 인프라 병목을 점검했다.

### 8-1. 평가 방법론

VWR Two-Tier 구조에 따라 각 구간의 **실제 부하 수준**을 먼저 식별한 뒤, 해당 부하에 대한 인프라 리소스 적정성을 평가했다.

```
구간별 실제 부하:
  브라우징 (15K)  → ISR 캐시로 backend 부하 최소
  VWR Tier 1      → Lambda가 15K 폴링 직접 처리
  Queue Tier 2    → 500-1000명 대기열 관리
  Backend         → 50-200명 동시 예매/결제
  Stats           → 비동기 Kafka 이벤트 처리
```

### 8-2. 발견 사항 (6개)

#### CRITICAL — 수정 완료

**1. Lambda reserved_concurrent_executions = 100 (→ 500으로 수정)**

15K 유저가 VWR에 진입하면 Lambda가 assign/check 요청을 처리한다. Lambda 1회 처리 ~50ms, 폴링 간격 2s 기준으로 동시 ~375 Lambda 실행이 필요하다.

- 이전: `reserved_concurrent_executions = 100` → 15K 중 **99.3% 거부**
- 수정: `terraform/modules/lambda-vwr/variables.tf:31` default 100 → 500
- 수정: `terraform/environments/prod/main.tf:303` override 100 → 500
- AWS 기본 계정 한도 1,000이므로 500은 한도 내

**2. catalog-service ticket_types 인덱스 누락 (→ V2 마이그레이션 추가)**

이벤트 상세 페이지에서 `SELECT * FROM ticket_types WHERE event_id = ?`를 실행한다. 인덱스 없이 full table scan이 발생하면 15K 브라우징에서 DB CPU 과부하를 유발한다.

- 이전: FK만 존재, 인덱스 없음 (`V1__catalog_core_schema.sql`)
- 수정: `services-spring/catalog-service/src/main/resources/db/migration/V2__add_ticket_types_index.sql` 추가
- 내용: `CREATE INDEX IF NOT EXISTS idx_ticket_types_event_id ON ticket_types(event_id);`
- 참고: PostgreSQL은 FK 컬럼에 자동 인덱스를 생성하지 않음 (MySQL과 다름)

**3. queue-service CPU request = 100m (→ 250m으로 수정)**

VWR 통과 500명이 2초 간격 폴링하면 ~250 req/s가 queue-service에 도달한다. JVM G1GC + Spring WebMVC 기본 오버헤드만으로 100m에 근접하며, 요청 처리 시 CPU throttling이 발생한다.

- 이전: `cpu: "100m"`, `memory: "256Mi"` (`k8s/spring/base/queue-service/deployment.yaml:60-61`)
- 수정: `cpu: "250m"`, `memory: "384Mi"`
- limits는 기존 유지 (`cpu: "500m"`, `memory: "512Mi"`)
- HPA가 있어도 base 리소스가 너무 낮으면 스케일업 반응(30-60초) 전에 장애 발생

#### CONCERN — 현재 상태 적절, 변경 불필요

**4. ISR 미적용 → 실제로는 이미 적용됨**

코드 확인 결과 주요 페이지에 ISR이 이미 구현되어 있었다:

```typescript
// apps/web/src/app/events/[id]/page.tsx
const res = await fetch(`${INTERNAL_API_URL}/api/v1/events/${id}`, {
  next: { revalidate: 60 },  // ISR 60초 캐시
});

// apps/web/src/app/page.tsx (홈)
const res = await fetch(`${INTERNAL_API_URL}/api/v1/events?status=on_sale`, {
  next: { revalidate: 60 },  // ISR 60초 캐시
});
```

15K 브라우징 시 첫 번째 요청만 SSR → catalog-service 호출, 이후 60초간 Next.js가 캐시된 HTML을 응답한다. 이벤트 수가 제한적(수십~수백)이므로 캐시 워밍이 즉시 완료된다. `generateStaticParams` 미사용이지만 ISR이 동일 효과를 제공한다.

**5. Kafka `acks: all` 지연 → 변경하면 안 됨**

| 서비스 | acks | 용도 |
|--------|------|------|
| payment-service | `all` (명시) | 결제 이벤트 발행 |
| ticket-service | `all` (명시) | 예약/좌석 이벤트 발행 |
| stats-service | `all` (Kafka 3.0+ 기본값) | DLQ 발행 전용 |

`acks: all` → `acks: 1` 변경 시 브로커 장애에서 결제 이벤트 손실 → 돈을 받고 좌석이 미확정되는 치명적 문제가 발생한다. VWR로 실제 백엔드 부하가 50-200명이므로 `acks: all` 추가 지연(~5ms)은 무시 가능하다.

**6. stats-service 단건 처리 → VWR 스케일에서 충분**

현재: `@KafkaListener`로 메시지 1건씩 수신, UPSERT 1-2회 실행.

실제 부하 계산:
- VWR 통과 200명 동시 예매 → 각 예매마다 reservation-event + payment-event = ~400 events/min
- ~7 events/sec × UPSERT ~2ms = **14ms/sec** (CPU 1.4%)

배치 전환 시 `BatchListenerFailedException`, deduplication 로직 변경 등 복잡도가 크게 증가하지만, 절감 효과는 14ms → 5ms로 무의미하다. 현재 단건 처리가 VWR 50-200명 스케일에 적합하다.

### 8-3. 수정 요약

| # | 이슈 | 심각도 | 조치 | 파일 |
|---|------|--------|------|------|
| 1 | Lambda 동시실행 100 | CRITICAL | 500으로 상향 | `terraform/modules/lambda-vwr/variables.tf`, `terraform/environments/prod/main.tf` |
| 2 | ticket_types 인덱스 누락 | CRITICAL | V2 마이그레이션 추가 | `catalog-service/.../V2__add_ticket_types_index.sql` |
| 3 | queue-service CPU 100m | CRITICAL | 250m으로 상향 | `k8s/spring/base/queue-service/deployment.yaml` |
| 4 | ISR 미적용 | 오탐 | 이미 `revalidate: 60` 적용됨 | 변경 없음 |
| 5 | Kafka acks: all | 정상 | 데이터 무결성상 유지 필수 | 변경 없음 |
| 6 | stats 배치 처리 | 불필요 | 7 events/sec에 오버엔지니어링 | 변경 없음 |

---

## 9. 평가

### 9-1. 잘된 점

**인프라 설계**
- 5-티어 서브넷 분리 (public/app/db/cache/streaming)로 네트워크 보안이 체계적이다 (`modules/vpc/main.tf:1-9`).
- AZ별 NAT Gateway 배치로 고가용성을 확보했다 (`modules/vpc/main.tf:96-105`).
- 9개 VPC Interface Endpoint + 2개 Gateway Endpoint로 VPC 내부 트래픽을 최적화했다 (`modules/vpc-endpoints/main.tf:46-198`).

**보안**
- WAF에 Rate Limiting + 3개 AWS Managed Rule Set을 적용했다 (`modules/waf/main.tf:25-114`).
- CloudFront -> ALB 커스텀 헤더 검증으로 ALB 직접 접근을 차단한다 (`modules/cloudfront/main.tf:102-105`).
- K8s NetworkPolicy로 서비스 간 통신을 세밀하게 제어한다 (`k8s/spring/base/network-policies.yaml`).
- 모든 데이터 저장소 암호화: RDS(`storage_encrypted=true`), Redis(`at_rest_encryption_enabled=true`), MSK(`encryption_at_rest`), S3(AES256), SQS(SSE).
- Lambda@Edge 시크릿 관리를 config.json 빌드 타임 주입으로 해결했다.
- securityContext로 Pod 권한 최소화: `runAsNonRoot`, `drop ALL capabilities` (`k8s/spring/base/ticket-service/deployment.yaml:22-35`).

**운영**
- Terraform 모듈화가 20개 모듈로 잘 분리되어 환경별 재사용이 용이하다.
- Staging/Prod 환경 차이를 변수로 명확히 관리한다 (Multi-AZ, Read Replica, 인스턴스 크기 등).
- Argo Rollouts Blue-Green + AnalysisTemplate으로 안전한 프로덕션 배포를 구현했다.
- SQS DLQ + CloudWatch 알람으로 메시지 처리 실패를 모니터링한다.
- ECR 이미지 태그 IMMUTABLE + 보안 스캔으로 이미지 무결성을 보장한다 (`modules/ecr/main.tf:13-17`).

**CI/CD**
- Reusable Workflow 패턴으로 8개 서비스의 파이프라인을 DRY하게 관리한다.
- OIDC 기반 AWS 인증으로 장기 자격증명을 제거했다 (`reusable-spring-ci-cd.yml:152-156`).
- Trivy 보안 스캔을 빌드 파이프라인에 통합했다.
- PR Validation에서 변경된 서비스만 병렬 테스트한다.
- 5회 재시도 Push로 동시 커밋 충돌을 처리한다.

### 9-2. 미흡한 점

**보안 우려**
- EKS API 서버 퍼블릭 엔드포인트가 `0.0.0.0/0`으로 열려 있다 (`prod/variables.tf:52`). VPN/Bastion 없이 인터넷에서 직접 접근 가능하다.
- KMS 키가 null로 설정되어 EKS Secrets 암호화에 AWS 관리형 키를 사용한다 (`prod/variables.tf:58`). 별도 KMS CMK를 사용하는 것이 좋다.
- RDS Proxy IAM 인증이 `DISABLED`이다 (`modules/rds/main.tf:155`). Secrets Manager 기반이지만 IAM 인증이 더 안전하다.

**MSK 구성**
- 브로커 2대에 `min.insync.replicas=1`이므로 1대 장애 시 데이터 유실 가능성이 있다 (`modules/msk/main.tf:101`). 프로덕션에서 브로커 3대 + `min.insync.replicas=2`를 권장한다.
- `auto.create.topics.enable=true`는 오타에 의한 토픽 생성 위험이 있다 (`main.tf:99`).

**고가용성 부족**
- Staging에서 RDS Single-AZ, Redis 1노드로 운영한다. 스테이징이 프로덕션과 동일 아키텍처가 아니면 환경 차이로 인한 장애를 사전에 발견하기 어렵다.
- catalog-service에 대한 HPA가 없다. 읽기 트래픽이 많을 수 있는 서비스임에도 오토스케일링이 적용되지 않았다.

**모니터링 부족**
- AMP/AMG 인프라는 구성했으나, 실제 Helm 차트 배포나 대시보드/알림 규칙이 코드로 관리되지 않는다.
- RDS, ElastiCache에 대한 CloudWatch 알람이 정의되어 있지 않다. MSK와 SQS에만 알람이 있다.

**CI/CD**
- Kustomize 이미지 태그 업데이트를 `sed`로 처리한다 (`reusable-spring-ci-cd.yml:245-248`). `kustomize edit set image` 명령어가 더 안전하다.
- `[skip ci]` 커밋이 Git 히스토리를 오염시킨다. 별도 GitOps 레포지토리 분리를 고려할 수 있다.
- Argo Rollouts의 `autoPromotionEnabled: false`이지만, ArgoCD의 `automated.selfHeal: true`와의 상호작용이 명확하지 않다.

### 9-3. AWS 배포 시 보완 가능한 점

1. **EKS 퍼블릭 엔드포인트 제한**: CIDR을 사무실/VPN IP로 제한하거나, 퍼블릭 엔드포인트를 비활성화하고 Private Access만 사용 (`prod/variables.tf:46-52`).

2. **MSK 브로커 3대**: `number_of_broker_nodes = 3`으로 변경하여 `min.insync.replicas=2`를 활성화 (`prod/main.tf:181`).

3. **KMS CMK 적용**: EKS Secrets, RDS, MSK, S3에 고객 관리형 KMS 키를 적용하여 키 로테이션 관리.

4. **CloudWatch 알람 확충**: RDS CPUUtilization, FreeableMemory, DatabaseConnections, ElastiCache CacheHitRate, EngineCPUUtilization에 대한 알람 추가.

5. **Secrets Manager 자동 로테이션**: RDS 자격증명과 Redis AUTH 토큰에 대한 Lambda 기반 자동 로테이션 구성.

6. **WAF 로깅**: WAF 로그를 S3 또는 CloudWatch로 전송하여 차단된 요청 분석 가능하도록 구성.

7. **Staging CloudFront**: Staging에서도 CloudFront를 사용하여 프로덕션과 동일한 트래픽 경로를 테스트할 수 있도록 구성 (현재 Staging은 ALB 직접 접근).

8. **GitOps 레포지토리 분리**: 애플리케이션 코드와 K8s 매니페스트를 별도 레포지토리로 분리하면 `[skip ci]` 커밋 문제를 해결하고, 배포 이력을 독립적으로 관리할 수 있다.

9. **Terraform State 접근 제어**: State 버킷에 IAM 정책을 추가하여 특정 역할만 접근 가능하도록 제한.

10. **Cost Optimization**: Staging에서 SPOT 인스턴스를 이미 사용하고 있다 (`staging/variables.tf:88`). Prod에서도 비핵심 워크로드에 SPOT을 혼합 사용하거나, Karpenter의 consolidation 정책을 활용하여 비용을 절감할 수 있다.
